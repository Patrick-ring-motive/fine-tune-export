{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "execution": {
     "iopub.execute_input": "2024-06-24T18:28:35.170699Z",
     "iopub.status.busy": "2024-06-24T18:28:35.170562Z",
     "iopub.status.idle": "2024-06-24T18:28:47.485992Z",
     "shell.execute_reply": "2024-06-24T18:28:47.485296Z",
     "shell.execute_reply.started": "2024-06-24T18:28:35.170684Z"
    },
    "id": "GLXwJqbjtPho",
    "outputId": "3a58991f-493c-430b-adb0-0a7c8170c72a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'transformers' from '/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/__init__.py'>\n",
      "Successfully imported transformers.\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from metapython import *\n",
    "#!rm -rf /workspace/pring/llama-2-7b-chat-hf1*\n",
    "#!pip install -U transformers\n",
    "#!conda install --yes tokenizers\n",
    "#!pip install --upgrade pip\n",
    "#!pip install tokenizers==0.15.2\n",
    "#!git clone https://github.com/NVIDIA/apex\n",
    "#!pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --config-settings \"--build-option=--cpp_ext\" --config-settings \"--build-option=--cuda_ext\" ./apex/\n",
    "importLib(\"transformers\")\n",
    "#!pip install transformers==4.13.0\n",
    "!conda install --yes pytorch==2.1.1 torchvision==0.16.1 torchaudio==2.1.1 pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "#!conda install --yes transformers[deepspeed]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-06-24T18:28:47.487699Z",
     "iopub.status.busy": "2024-06-24T18:28:47.487365Z",
     "iopub.status.idle": "2024-06-24T18:28:48.080660Z",
     "shell.execute_reply": "2024-06-24T18:28:48.080150Z",
     "shell.execute_reply.started": "2024-06-24T18:28:47.487681Z"
    },
    "id": "FmhXEL79Jp42",
    "outputId": "b9769b9c-5de6-427a-a797-837b40bf310d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'datasets' from '/opt/conda/envs/pytorch/lib/python3.11/site-packages/datasets/__init__.py'>\n",
      "Successfully imported datasets.\n",
      "<module 'chardet' from '/opt/conda/envs/pytorch/lib/python3.11/site-packages/chardet/__init__.py'>\n",
      "Successfully imported chardet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'chardet' from '/opt/conda/envs/pytorch/lib/python3.11/site-packages/chardet/__init__.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "  import torch\n",
    "except:\n",
    "  !conda install --yes accelerate peft bitsandbytes transformers trl dataset torch==2.1.1\n",
    "  !conda install --yes torch==2.1.1\n",
    "import torch\n",
    "if (torch.cuda.is_available() == False ):\n",
    "  !conda install --yes torch==2.1.1\n",
    "  !conda install --yes accelerate peft bitsandbytes transformers trl dataset torch==2.1.1\n",
    "  import torch\n",
    "if (torch.cuda.is_available() == False ):\n",
    "    raise Exception(\"Reinstall pytorch\")\n",
    "    \n",
    "importLib(\"datasets\")\n",
    "importLib(\"chardet\")\n",
    "\n",
    "    \n",
    "#!conda install --yes huggingface_hub\n",
    "#!conda install --yes tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-24T18:28:48.081674Z",
     "iopub.status.busy": "2024-06-24T18:28:48.081385Z",
     "iopub.status.idle": "2024-06-24T18:28:48.235497Z",
     "shell.execute_reply": "2024-06-24T18:28:48.235049Z",
     "shell.execute_reply.started": "2024-06-24T18:28:48.081659Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-24T18:28:48.236835Z",
     "iopub.status.busy": "2024-06-24T18:28:48.236678Z",
     "iopub.status.idle": "2024-06-24T18:28:48.698665Z",
     "shell.execute_reply": "2024-06-24T18:28:48.697984Z",
     "shell.execute_reply.started": "2024-06-24T18:28:48.236821Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun 24 18:28:48 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          Off |   00000000:9B:00.0 Off |                    0 |\n",
      "| N/A   40C    P0            118W /  700W |   36656MiB /  38146MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-24T18:28:48.699806Z",
     "iopub.status.busy": "2024-06-24T18:28:48.699635Z",
     "iopub.status.idle": "2024-06-24T18:28:49.731139Z",
     "shell.execute_reply": "2024-06-24T18:28:49.730620Z",
     "shell.execute_reply.started": "2024-06-24T18:28:48.699787Z"
    },
    "id": "nAMzy_0FtaUZ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from datasets import load_dataset\n",
      "Successfully imported datasets.\n",
      "<function load_dataset at 0x7f31d5cda520>\n",
      "<module 'datasets' from '/opt/conda/envs/pytorch/lib/python3.11/site-packages/datasets/__init__.py'>\n",
      "Successfully imported datasets.\n",
      "from transformers import AutoModelForCausalLM\n",
      "Successfully imported transformers.\n",
      "<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>\n",
      "from transformers import AutoTokenizer\n",
      "Successfully imported transformers.\n",
      "<class 'transformers.models.auto.tokenization_auto.AutoTokenizer'>\n",
      "from transformers import BitsAndBytesConfig\n",
      "Successfully imported transformers.\n",
      "<class 'transformers.utils.quantization_config.BitsAndBytesConfig'>\n",
      "from transformers import HfArgumentParser\n",
      "Successfully imported transformers.\n",
      "<class 'transformers.hf_argparser.HfArgumentParser'>\n",
      "from transformers import TrainingArguments\n",
      "Successfully imported transformers.\n",
      "<class 'transformers.training_args.TrainingArguments'>\n",
      "from transformers import pipeline\n",
      "Successfully imported transformers.\n",
      "<function pipeline at 0x7f31d3e56fc0>\n",
      "from transformers import logging\n",
      "Successfully imported transformers.\n",
      "<module 'transformers.utils.logging' from '/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/utils/logging.py'>\n",
      "from peft import LoraConfig\n",
      "Successfully imported peft.\n",
      "<class 'peft.tuners.lora.config.LoraConfig'>\n",
      "from peft import PeftModel\n",
      "Successfully imported peft.\n",
      "<class 'peft.peft_model.PeftModel'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os,sys\n",
    "import torch\n",
    "fromImport(\"datasets\",[\"load_dataset\"])\n",
    "importLib(\"datasets\")\n",
    "fromImport(\"transformers\",[\"AutoModelForCausalLM\", \"AutoTokenizer\",\"BitsAndBytesConfig\",\"HfArgumentParser\",\"TrainingArguments\",\"pipeline\",\"logging\"])\n",
    "fromImport(\"peft\",[\"LoraConfig\", \"PeftModel\"])\n",
    "from trl import SFTTrainer\n",
    "import time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-24T18:28:49.732034Z",
     "iopub.status.busy": "2024-06-24T18:28:49.731868Z",
     "iopub.status.idle": "2024-06-24T18:28:49.735668Z",
     "shell.execute_reply": "2024-06-24T18:28:49.735176Z",
     "shell.execute_reply.started": "2024-06-24T18:28:49.732020Z"
    },
    "id": "04W4oeqScK0e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "#model_name = \"microsoft/phi-2\"\n",
    "#model_name = \"NousResearch/Llama-2-7b-chat-hf\" # The model that you want to train from the Hugging Face hub\n",
    "#model_name = \"openai-community/gpt2\"\n",
    "#model_name = \"Weblet/gpt2-medium-turbo1713756936088908_mlabonne-guanaco-llama2-1k_train\"\n",
    "#model_name = \"openai-community/gpt2-medium\"\n",
    "#model_name = \"microsoft/phi-1.5\"\n",
    "#model_name = \"Weblet/phi-1.5.2\"\n",
    "#model_name = \"microsoft/phi-1\"\n",
    "#model_name = \"/workspace/pring/llama-2-7b-chat-hf\"\n",
    "#model_name = \"Weblet/phi-1.5-turbo1713979458374441_mlabonne-guanaco-llama2-1k_train\"\n",
    "#model_name = \"lvkaokao/llama2-7b-hf-chat-lora-v3\"\n",
    "model_name = \"Weblet/Phi-3-mini-4k-instruct-turbo17192528440841563_-workspace-pring-fine-tune-cdataset_train\"\n",
    "\n",
    "model_splitname = model_name.split('/')\n",
    "model_subname = model_splitname[len(model_splitname)-1]\n",
    "dataset_name = \"mlabonne/guanaco-llama2-1k\" # The instruction dataset to use\n",
    "#dataset_name = \"mlabonne/orpo-dpo-mix-40k\"\n",
    "#dataset_name = \"cognitivecomputations/Code-290k-ShareGPT-Vicuna\"\n",
    "\n",
    "new_model = \"Weblet/\" + model_subname + \"-turbo\"+ f\"{time.time()}\".replace('.','')# Fine-tuned model name\n",
    "#new_model = \"/workspace/pring/\" + model_subname + f\"{time.time()}\".replace('.','')# Fine-tuned model name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-24T18:28:49.736688Z",
     "iopub.status.busy": "2024-06-24T18:28:49.736548Z",
     "iopub.status.idle": "2024-06-24T18:28:49.740946Z",
     "shell.execute_reply": "2024-06-24T18:28:49.740529Z",
     "shell.execute_reply.started": "2024-06-24T18:28:49.736675Z"
    },
    "id": "ib_We3NLtj2E",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"backend:cudaMallocAsync\"\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "lora_r = 64 # LoRA attention dimension\n",
    "lora_alpha = 16 # Alpha parameter for LoRA scaling\n",
    "lora_dropout = 0.1 # Dropout probability for LoRA layers\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "use_4bit = True # Activate 4-bit precision base model loading\n",
    "bnb_4bit_compute_dtype = \"float16\" # Compute dtype for 4-bit base models\n",
    "bnb_4bit_quant_type = \"nf4\" # Quantization type (fp4 or nf4)\n",
    "use_nested_quant = False # Activate nested quantization for 4-bit base models (double quantization)\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "output_dir = \"./results\" # Output directory where the model predictions and checkpoints will be stored\n",
    "num_train_epochs = 1 # Number of training epochs\n",
    "fp16 = False # Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "bf16 = False\n",
    "per_device_train_batch_size = 4 # Batch size per GPU for training\n",
    "per_device_eval_batch_size = 4 # Batch size per GPU for evaluation\n",
    "gradient_accumulation_steps = 1 # Number of update steps to accumulate the gradients for\n",
    "gradient_checkpointing = True # Enable gradient checkpointing\n",
    "max_grad_norm = 0.3 # Maximum gradient normal (gradient clipping)\n",
    "learning_rate = 2e-4 # Initial learning rate (AdamW optimizer)\n",
    "weight_decay = 0.001 # Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "optim = \"paged_adamw_32bit\" #\"adamw_apex_fused\"# Optimizer to use\n",
    "lr_scheduler_type = \"cosine\" # Learning rate schedule\n",
    "max_steps = -1 # Number of training steps (overrides num_train_epochs)\n",
    "warmup_ratio = 0.03 # Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "group_by_length = False # Group sequences into batches with same length # Saves memory and speeds up training considerably\n",
    "save_steps = 0 # Save checkpoint every X updates steps\n",
    "logging_steps = 25 # Log every X updates steps\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "max_seq_length = 256 # Maximum sequence length to use\n",
    "packing = False # Pack multiple short examples in the same input sequence to increase efficiency\n",
    "device_map = {\"\": 0} #  Load the entire model on the GPU 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "execution": {
     "iopub.execute_input": "2024-06-24T18:28:49.741669Z",
     "iopub.status.busy": "2024-06-24T18:28:49.741523Z",
     "iopub.status.idle": "2024-06-24T18:28:49.744955Z",
     "shell.execute_reply": "2024-06-24T18:28:49.744526Z",
     "shell.execute_reply.started": "2024-06-24T18:28:49.741655Z"
    },
    "id": "Q5EHNbUzCUNv",
    "outputId": "c534abfe-a809-44d6-f6df-ab422827058a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom datasets import ArrowBasedBuilder\\ndef globalThis():\\n  pass\\n\\ndef pss(*args, **kwargs):\\n  try:\\n    globalThis.ArrowBasedBuilder = ArrowBasedBuilder(*args, **kwargs)\\n  except:\\n    pass\\n  return globalThis.ArrowBasedBuilder\\n\\nArrowBasedBuilder._prepare_split_single = pss\\nprint(ArrowBasedBuilder._prepare_split_single)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from datasets import ArrowBasedBuilder\n",
    "def globalThis():\n",
    "  pass\n",
    "\n",
    "def pss(*args, **kwargs):\n",
    "  try:\n",
    "    globalThis.ArrowBasedBuilder = ArrowBasedBuilder(*args, **kwargs)\n",
    "  except:\n",
    "    pass\n",
    "  return globalThis.ArrowBasedBuilder\n",
    "\n",
    "ArrowBasedBuilder._prepare_split_single = pss\n",
    "print(ArrowBasedBuilder._prepare_split_single)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-24T18:28:49.745630Z",
     "iopub.status.busy": "2024-06-24T18:28:49.745500Z",
     "iopub.status.idle": "2024-06-24T18:28:51.735927Z",
     "shell.execute_reply": "2024-06-24T18:28:51.735409Z",
     "shell.execute_reply.started": "2024-06-24T18:28:49.745618Z"
    },
    "id": "VdcpiHdhgttR",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/datasets/load.py:2562: FutureWarning: 'ignore_verifications' was deprecated in favor of 'verification_mode' in version 2.9.1 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'verification_mode=no_checks' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load dataset (you can process it here)\n",
    "dataset_split = \"train\"\n",
    "#dataset_split = \"train[:1%]\"\n",
    "#dataset_split = \"train[5%:10%]\"\n",
    "#dataset_pre = load_dataset(dataset_name)\n",
    "try:\n",
    "  dataset_pre = load_dataset(dataset_name, split=dataset_split,encoding = 'UTF-8')\n",
    "except:\n",
    "  dataset_pre = load_dataset(dataset_name, split=dataset_split,ignore_verifications=True,verification_mode=\"no_checks\")\n",
    "dataset = dataset_pre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-06-24T18:28:51.737848Z",
     "iopub.status.busy": "2024-06-24T18:28:51.737687Z",
     "iopub.status.idle": "2024-06-24T18:28:51.741409Z",
     "shell.execute_reply": "2024-06-24T18:28:51.740955Z",
     "shell.execute_reply.started": "2024-06-24T18:28:51.737834Z"
    },
    "id": "ZGUkumL7ZKpw",
    "outputId": "9c7d42e0-4454-4d86-d44c-22437415f574",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if dataset_name == \"cognitivecomputations/Code-290k-ShareGPT-Vicuna\":\n",
    "  dataset_pre=dataset_pre.rename_column('conversations','text')\n",
    "  dataset_pre.set_format(type= None, format_kwargs= {}, columns= ['text'], output_all_columns= False)\n",
    "  def toPhi(data):\n",
    "    data['text']=\"<s>[INST]\"+data['text'][0]['value']+\"[/INST]\"+data['text'][1]['value']+\"</s>\"\n",
    "    return data\n",
    "  dataset = dataset_pre.map(toPhi)\n",
    "\n",
    "  print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-06-24T18:28:51.742207Z",
     "iopub.status.busy": "2024-06-24T18:28:51.742043Z",
     "iopub.status.idle": "2024-06-24T18:29:04.702479Z",
     "shell.execute_reply": "2024-06-24T18:29:04.701614Z",
     "shell.execute_reply.started": "2024-06-24T18:28:51.742194Z"
    },
    "id": "rW9PDhEllYvk",
    "outputId": "f03a21eb-4372-43d8-db4c-066ab1b7d266",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/runai-home/.cache/huggingface/token\n",
      "Login successful\n",
      "True\n",
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n",
      "{'': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:08<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 376.00 MiB. GPU 0 has a total capacty of 37.25 GiB of which 0 bytes is free. Process 48685 has 26.70 GiB memory in use. Process 54879 has 9.09 GiB memory in use. Including non-PyTorch memory, this process has 1.46 GiB memory in use. Of the allocated memory 793.51 MiB is allocated by PyTorch, and 92.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 30\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m   model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name,quantization_config\u001b[38;5;241m=\u001b[39mbnb_config,device_map\u001b[38;5;241m=\u001b[39mdevice_map, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    562\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/modeling_utils.py:3850\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3842\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3843\u001b[0m     (\n\u001b[1;32m   3844\u001b[0m         model,\n\u001b[1;32m   3845\u001b[0m         missing_keys,\n\u001b[1;32m   3846\u001b[0m         unexpected_keys,\n\u001b[1;32m   3847\u001b[0m         mismatched_keys,\n\u001b[1;32m   3848\u001b[0m         offload_index,\n\u001b[1;32m   3849\u001b[0m         error_msgs,\n\u001b[0;32m-> 3850\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_load_pretrained_model(\n\u001b[1;32m   3851\u001b[0m         model,\n\u001b[1;32m   3852\u001b[0m         state_dict,\n\u001b[1;32m   3853\u001b[0m         loaded_state_dict_keys,  \u001b[38;5;66;03m# XXX: rename?\u001b[39;00m\n\u001b[1;32m   3854\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3855\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3856\u001b[0m         ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39mignore_mismatched_sizes,\n\u001b[1;32m   3857\u001b[0m         sharded_metadata\u001b[38;5;241m=\u001b[39msharded_metadata,\n\u001b[1;32m   3858\u001b[0m         _fast_init\u001b[38;5;241m=\u001b[39m_fast_init,\n\u001b[1;32m   3859\u001b[0m         low_cpu_mem_usage\u001b[38;5;241m=\u001b[39mlow_cpu_mem_usage,\n\u001b[1;32m   3860\u001b[0m         device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[1;32m   3861\u001b[0m         offload_folder\u001b[38;5;241m=\u001b[39moffload_folder,\n\u001b[1;32m   3862\u001b[0m         offload_state_dict\u001b[38;5;241m=\u001b[39moffload_state_dict,\n\u001b[1;32m   3863\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mtorch_dtype,\n\u001b[1;32m   3864\u001b[0m         is_quantized\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES),\n\u001b[1;32m   3865\u001b[0m         keep_in_fp32_modules\u001b[38;5;241m=\u001b[39mkeep_in_fp32_modules,\n\u001b[1;32m   3866\u001b[0m     )\n\u001b[1;32m   3868\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_4bit \u001b[38;5;241m=\u001b[39m load_in_4bit\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/modeling_utils.py:4284\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   4283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4284\u001b[0m     new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m _load_state_dict_into_meta_model(\n\u001b[1;32m   4285\u001b[0m         model_to_load,\n\u001b[1;32m   4286\u001b[0m         state_dict,\n\u001b[1;32m   4287\u001b[0m         loaded_keys,\n\u001b[1;32m   4288\u001b[0m         start_prefix,\n\u001b[1;32m   4289\u001b[0m         expected_keys,\n\u001b[1;32m   4290\u001b[0m         device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[1;32m   4291\u001b[0m         offload_folder\u001b[38;5;241m=\u001b[39moffload_folder,\n\u001b[1;32m   4292\u001b[0m         offload_index\u001b[38;5;241m=\u001b[39moffload_index,\n\u001b[1;32m   4293\u001b[0m         state_dict_folder\u001b[38;5;241m=\u001b[39mstate_dict_folder,\n\u001b[1;32m   4294\u001b[0m         state_dict_index\u001b[38;5;241m=\u001b[39mstate_dict_index,\n\u001b[1;32m   4295\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   4296\u001b[0m         is_quantized\u001b[38;5;241m=\u001b[39mis_quantized,\n\u001b[1;32m   4297\u001b[0m         is_safetensors\u001b[38;5;241m=\u001b[39mis_safetensors,\n\u001b[1;32m   4298\u001b[0m         keep_in_fp32_modules\u001b[38;5;241m=\u001b[39mkeep_in_fp32_modules,\n\u001b[1;32m   4299\u001b[0m         unexpected_keys\u001b[38;5;241m=\u001b[39munexpected_keys,\n\u001b[1;32m   4300\u001b[0m     )\n\u001b[1;32m   4301\u001b[0m     error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/modeling_utils.py:839\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, is_quantized, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001b[0m\n\u001b[1;32m    837\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    838\u001b[0m         \u001b[38;5;66;03m# loading not quantized params in quantized model\u001b[39;00m\n\u001b[0;32m--> 839\u001b[0m         set_module_quantized_tensor_to_device(model, param_name, param_device, value\u001b[38;5;241m=\u001b[39mparam)\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m error_msgs, offload_index, state_dict_index\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/integrations/bitsandbytes.py:121\u001b[0m, in \u001b[0;36mset_module_quantized_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, quantized_stats)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 121\u001b[0m         new_value \u001b[38;5;241m=\u001b[39m bnb\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mParams4bit(new_value, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    122\u001b[0m module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m new_value\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:324\u001b[0m, in \u001b[0;36mParams4bit.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbnb_quantized:\n\u001b[0;32m--> 324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_quantize(device)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:289\u001b[0m, in \u001b[0;36mParams4bit._quantize\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    288\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mcuda(device)\n\u001b[0;32m--> 289\u001b[0m w_4bit, quant_state \u001b[38;5;241m=\u001b[39m bnb\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mquantize_4bit(\n\u001b[1;32m    290\u001b[0m     w,\n\u001b[1;32m    291\u001b[0m     blocksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize,\n\u001b[1;32m    292\u001b[0m     compress_statistics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompress_statistics,\n\u001b[1;32m    293\u001b[0m     quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_type,\n\u001b[1;32m    294\u001b[0m     quant_storage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_storage,\n\u001b[1;32m    295\u001b[0m )\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m w_4bit\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/bitsandbytes/functional.py:1165\u001b[0m, in \u001b[0;36mquantize_4bit\u001b[0;34m(A, absmax, out, blocksize, compress_statistics, quant_type, quant_storage)\u001b[0m\n\u001b[1;32m   1164\u001b[0m     blocks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m%\u001b[39m blocksize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1165\u001b[0m     absmax \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((blocks,), device\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 37.25 GiB of which 0 bytes is free. Process 48685 has 26.70 GiB memory in use. Process 54879 has 9.09 GiB memory in use. Including non-PyTorch memory, this process has 1.46 GiB memory in use. Of the allocated memory 793.51 MiB is allocated by PyTorch, and 92.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m   model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name,quantization_config\u001b[38;5;241m=\u001b[39mbnb_config,device_map\u001b[38;5;241m=\u001b[39mdevice_map, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m   model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name,device_map\u001b[38;5;241m=\u001b[39mdevice_map, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     33\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     34\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    562\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/modeling_utils.py:3850\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3841\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3842\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3843\u001b[0m     (\n\u001b[1;32m   3844\u001b[0m         model,\n\u001b[1;32m   3845\u001b[0m         missing_keys,\n\u001b[1;32m   3846\u001b[0m         unexpected_keys,\n\u001b[1;32m   3847\u001b[0m         mismatched_keys,\n\u001b[1;32m   3848\u001b[0m         offload_index,\n\u001b[1;32m   3849\u001b[0m         error_msgs,\n\u001b[0;32m-> 3850\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_load_pretrained_model(\n\u001b[1;32m   3851\u001b[0m         model,\n\u001b[1;32m   3852\u001b[0m         state_dict,\n\u001b[1;32m   3853\u001b[0m         loaded_state_dict_keys,  \u001b[38;5;66;03m# XXX: rename?\u001b[39;00m\n\u001b[1;32m   3854\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3855\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3856\u001b[0m         ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39mignore_mismatched_sizes,\n\u001b[1;32m   3857\u001b[0m         sharded_metadata\u001b[38;5;241m=\u001b[39msharded_metadata,\n\u001b[1;32m   3858\u001b[0m         _fast_init\u001b[38;5;241m=\u001b[39m_fast_init,\n\u001b[1;32m   3859\u001b[0m         low_cpu_mem_usage\u001b[38;5;241m=\u001b[39mlow_cpu_mem_usage,\n\u001b[1;32m   3860\u001b[0m         device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[1;32m   3861\u001b[0m         offload_folder\u001b[38;5;241m=\u001b[39moffload_folder,\n\u001b[1;32m   3862\u001b[0m         offload_state_dict\u001b[38;5;241m=\u001b[39moffload_state_dict,\n\u001b[1;32m   3863\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mtorch_dtype,\n\u001b[1;32m   3864\u001b[0m         is_quantized\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES),\n\u001b[1;32m   3865\u001b[0m         keep_in_fp32_modules\u001b[38;5;241m=\u001b[39mkeep_in_fp32_modules,\n\u001b[1;32m   3866\u001b[0m     )\n\u001b[1;32m   3868\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_4bit \u001b[38;5;241m=\u001b[39m load_in_4bit\n\u001b[1;32m   3869\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_8bit \u001b[38;5;241m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/modeling_utils.py:4284\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   4280\u001b[0m                     set_module_quantized_tensor_to_device(\n\u001b[1;32m   4281\u001b[0m                         model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   4282\u001b[0m                     )\n\u001b[1;32m   4283\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4284\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m _load_state_dict_into_meta_model(\n\u001b[1;32m   4285\u001b[0m             model_to_load,\n\u001b[1;32m   4286\u001b[0m             state_dict,\n\u001b[1;32m   4287\u001b[0m             loaded_keys,\n\u001b[1;32m   4288\u001b[0m             start_prefix,\n\u001b[1;32m   4289\u001b[0m             expected_keys,\n\u001b[1;32m   4290\u001b[0m             device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[1;32m   4291\u001b[0m             offload_folder\u001b[38;5;241m=\u001b[39moffload_folder,\n\u001b[1;32m   4292\u001b[0m             offload_index\u001b[38;5;241m=\u001b[39moffload_index,\n\u001b[1;32m   4293\u001b[0m             state_dict_folder\u001b[38;5;241m=\u001b[39mstate_dict_folder,\n\u001b[1;32m   4294\u001b[0m             state_dict_index\u001b[38;5;241m=\u001b[39mstate_dict_index,\n\u001b[1;32m   4295\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   4296\u001b[0m             is_quantized\u001b[38;5;241m=\u001b[39mis_quantized,\n\u001b[1;32m   4297\u001b[0m             is_safetensors\u001b[38;5;241m=\u001b[39mis_safetensors,\n\u001b[1;32m   4298\u001b[0m             keep_in_fp32_modules\u001b[38;5;241m=\u001b[39mkeep_in_fp32_modules,\n\u001b[1;32m   4299\u001b[0m             unexpected_keys\u001b[38;5;241m=\u001b[39munexpected_keys,\n\u001b[1;32m   4300\u001b[0m         )\n\u001b[1;32m   4301\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/modeling_utils.py:805\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, is_quantized, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001b[0m\n\u001b[1;32m    802\u001b[0m     state_dict_index \u001b[38;5;241m=\u001b[39m offload_weight(param, param_name, state_dict_folder, state_dict_index)\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized:\n\u001b[1;32m    804\u001b[0m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate`\u001b[39;00m\n\u001b[0;32m--> 805\u001b[0m     set_module_tensor_to_device(model, param_name, param_device, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mset_module_kwargs)\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01min\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39mint8, torch\u001b[38;5;241m.\u001b[39muint8) \u001b[38;5;129;01mand\u001b[39;00m is_quantized:\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;66;03m# handling newly quantized weights and loaded quantized weights\u001b[39;00m\n\u001b[1;32m    808\u001b[0m     \u001b[38;5;66;03m# edit the param.dtype restrictions and is_quantized condition when adding new quant methods\u001b[39;00m\n\u001b[1;32m    809\u001b[0m     quantized_stats \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/accelerate/utils/modeling.py:400\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    398\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 400\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 376.00 MiB. GPU 0 has a total capacty of 37.25 GiB of which 0 bytes is free. Process 48685 has 26.70 GiB memory in use. Process 54879 has 9.09 GiB memory in use. Including non-PyTorch memory, this process has 1.46 GiB memory in use. Of the allocated memory 793.51 MiB is allocated by PyTorch, and 92.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "!huggingface-cli login --token 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\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "print(use_4bit)\n",
    "# Check GPU compatibility with bfloat16\n",
    "try:\n",
    "  if compute_dtype == torch.float16 and use_4bit:\n",
    "      major, _ = torch.cuda.get_device_capability()\n",
    "      if major >= 8:\n",
    "          print(\"=\" * 80)\n",
    "          print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "          print(\"=\" * 80)\n",
    "except:\n",
    "  device_map = \"cpu\"\n",
    "\n",
    "print(device_map)\n",
    "# Load base model\n",
    "try:\n",
    "  model = AutoModelForCausalLM.from_pretrained(model_name,quantization_config=bnb_config,device_map=device_map, trust_remote_code=True)\n",
    "except:\n",
    "  model = AutoModelForCausalLM.from_pretrained(model_name,device_map=device_map, trust_remote_code=True)\n",
    "model.config.use_cache = True\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.status.busy": "2024-06-24T18:29:04.703272Z",
     "iopub.status.idle": "2024-06-24T18:29:04.703484Z",
     "shell.execute_reply": "2024-06-24T18:29:04.703385Z",
     "shell.execute_reply.started": "2024-06-24T18:29:04.703374Z"
    },
    "id": "XW94Kylolc_r",
    "outputId": "82e86c0a-43b0-4874-e2f8-255480db71b4"
   },
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "#logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "\"\"\"prompt = \"At what temperature does water boil?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=512)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "631e20ee4a0e4cfeae991cf1fea069a1",
      "8ee669f3cd504bb98648d0c9c598aa37",
      "e93819afef394d48ad16540e2cd14b2d",
      "ad14f7ec5d5d43cd8983c6f841f4092e",
      "f3ed89e0408c480fa75458b3aecc8248",
      "6d06606c1e3c4ccfa1335139fdcd43e0",
      "1024f48c3eb74964bb03b8fbb46ccd60",
      "5af754a056224ddb9496808fced259c0",
      "e4d9759289e541158a45e6967b0c4c58",
      "5dd1405ef5594f9f9daa5a9730611335",
      "aa0855f52c554bdebb1235a1dd7dbba6"
     ]
    },
    "execution": {
     "iopub.status.busy": "2024-06-24T18:29:04.704316Z",
     "iopub.status.idle": "2024-06-24T18:29:04.704497Z",
     "shell.execute_reply": "2024-06-24T18:29:04.704418Z",
     "shell.execute_reply.started": "2024-06-24T18:29:04.704409Z"
    },
    "id": "OJXpOgBFuSrc",
    "outputId": "178afa88-f096-4989-ac56-1fa294d73285",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "try:\n",
    "  trainer.train()\n",
    "except Exception as e: \n",
    "  new_model = new_model\n",
    "  print(e)\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-24T18:29:04.705037Z",
     "iopub.status.idle": "2024-06-24T18:29:04.705212Z",
     "shell.execute_reply": "2024-06-24T18:29:04.705140Z",
     "shell.execute_reply.started": "2024-06-24T18:29:04.705132Z"
    },
    "id": "crj9svNe4hU5"
   },
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir results/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "execution": {
     "iopub.status.busy": "2024-06-24T18:29:04.705741Z",
     "iopub.status.idle": "2024-06-24T18:29:04.705906Z",
     "shell.execute_reply": "2024-06-24T18:29:04.705828Z",
     "shell.execute_reply.started": "2024-06-24T18:29:04.705820Z"
    },
    "id": "frlSLPin4IJ4",
    "outputId": "976da46d-26f6-459c-bb8d-487a103c96a8"
   },
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"At what temperature does water boil?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=512)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-24T18:29:04.706396Z",
     "iopub.status.idle": "2024-06-24T18:29:04.706564Z",
     "shell.execute_reply": "2024-06-24T18:29:04.706496Z",
     "shell.execute_reply.started": "2024-06-24T18:29:04.706488Z"
    },
    "id": "mkQCviG0Zta-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Empty VRAM\n",
    "del model\n",
    "del pipe\n",
    "del trainer\n",
    "import gc\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "execution": {
     "iopub.status.busy": "2024-06-24T18:29:04.707110Z",
     "iopub.status.idle": "2024-06-24T18:29:04.707272Z",
     "shell.execute_reply": "2024-06-24T18:29:04.707200Z",
     "shell.execute_reply.started": "2024-06-24T18:29:04.707192Z"
    },
    "id": "QQn30cRtAZ-P",
    "outputId": "6d9eb287-d276-4da3-a469-6ff2af448481",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "print(type(new_model))\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-24T18:29:04.707966Z",
     "iopub.status.idle": "2024-06-24T18:29:04.708139Z",
     "shell.execute_reply": "2024-06-24T18:29:04.708059Z",
     "shell.execute_reply.started": "2024-06-24T18:29:04.708050Z"
    },
    "id": "zHLKfDGzsElK"
   },
   "outputs": [],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "prompt = \"At what temperature does water boil?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=512)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-24T18:29:04.708663Z",
     "iopub.status.idle": "2024-06-24T18:29:04.708820Z",
     "shell.execute_reply": "2024-06-24T18:29:04.708750Z",
     "shell.execute_reply.started": "2024-06-24T18:29:04.708742Z"
    },
    "id": "x-xPb-_qB0dz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "!huggingface-cli login --token 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\n",
    "\n",
    "model.push_to_hub((new_model+\"_\"+dataset_name.replace('/','-')+\"_\"+dataset_split.replace('[','').replace(']','').replace(':','').replace('%',''))[0:95], use_temp_dir=True)\n",
    "tokenizer.push_to_hub((new_model+\"_\"+dataset_name.replace('/','-')+\"_\"+dataset_split.replace('[','').replace(']','').replace(':','').replace('%',''))[0:95], use_temp_dir=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1024f48c3eb74964bb03b8fbb46ccd60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5af754a056224ddb9496808fced259c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5dd1405ef5594f9f9daa5a9730611335": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "631e20ee4a0e4cfeae991cf1fea069a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8ee669f3cd504bb98648d0c9c598aa37",
       "IPY_MODEL_e93819afef394d48ad16540e2cd14b2d",
       "IPY_MODEL_ad14f7ec5d5d43cd8983c6f841f4092e"
      ],
      "layout": "IPY_MODEL_f3ed89e0408c480fa75458b3aecc8248"
     }
    },
    "6d06606c1e3c4ccfa1335139fdcd43e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ee669f3cd504bb98648d0c9c598aa37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d06606c1e3c4ccfa1335139fdcd43e0",
      "placeholder": "",
      "style": "IPY_MODEL_1024f48c3eb74964bb03b8fbb46ccd60",
      "value": "Map:100%"
     }
    },
    "aa0855f52c554bdebb1235a1dd7dbba6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad14f7ec5d5d43cd8983c6f841f4092e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5dd1405ef5594f9f9daa5a9730611335",
      "placeholder": "",
      "style": "IPY_MODEL_aa0855f52c554bdebb1235a1dd7dbba6",
      "value": "28909/28909[00:42&lt;00:00,751.86examples/s]"
     }
    },
    "e4d9759289e541158a45e6967b0c4c58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e93819afef394d48ad16540e2cd14b2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5af754a056224ddb9496808fced259c0",
      "max": 28909,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e4d9759289e541158a45e6967b0c4c58",
      "value": 28909
     }
    },
    "f3ed89e0408c480fa75458b3aecc8248": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
