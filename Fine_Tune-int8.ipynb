{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "execution": {
     "iopub.execute_input": "2024-04-30T17:26:39.033101Z",
     "iopub.status.busy": "2024-04-30T17:26:39.032806Z",
     "iopub.status.idle": "2024-04-30T17:26:52.869417Z",
     "shell.execute_reply": "2024-04-30T17:26:52.867862Z",
     "shell.execute_reply.started": "2024-04-30T17:26:39.033062Z"
    },
    "id": "GLXwJqbjtPho",
    "outputId": "3a58991f-493c-430b-adb0-0a7c8170c72a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "git: 'lfs' is not a git command. See 'git --help'.\n",
      "\n",
      "The most similar command is\n",
      "\tlog\n",
      "git: 'lfs' is not a git command. See 'git --help'.\n",
      "\n",
      "The most similar command is\n",
      "\tlog\n",
      "Requirement already satisfied: ctranslate2 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (4.2.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from ctranslate2) (66.0.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from ctranslate2) (1.24.3)\n",
      "Requirement already satisfied: pyyaml<7,>=5.3 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from ctranslate2) (6.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!rm -rf /workspace/pring/llama-2-7b-chat-hf1*\n",
    "!conda install --yes pytorch==2.1.1 torchvision==0.16.1 torchaudio==2.1.1 pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "!git lfs install\n",
    "!git lfs pull\n",
    "!pip install ctranslate2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-30T17:26:52.873732Z",
     "iopub.status.busy": "2024-04-30T17:26:52.873058Z",
     "iopub.status.idle": "2024-04-30T17:27:14.081540Z",
     "shell.execute_reply": "2024-04-30T17:27:14.079980Z",
     "shell.execute_reply.started": "2024-04-30T17:26:52.873679Z"
    },
    "id": "FmhXEL79Jp42",
    "outputId": "b9769b9c-5de6-427a-a797-837b40bf310d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  import torch\n",
    "except:\n",
    "  !conda install --yes accelerate peft bitsandbytes transformers trl dataset torch==2.1.1\n",
    "  !conda install --yes torch==2.1.1\n",
    "import torch\n",
    "if (torch.cuda.is_available() == False ):\n",
    "  !conda install --yes torch==2.1.1\n",
    "  !conda install --yes accelerate peft bitsandbytes transformers trl dataset torch==2.1.1\n",
    "  import torch\n",
    "if (torch.cuda.is_available() == False ):\n",
    "    raise Exception(\"Reinstall pytorch\")\n",
    "    \n",
    "try:\n",
    "  import datasets\n",
    "except:\n",
    "  !conda install --yes datasets\n",
    "  import datasets\n",
    "    \n",
    "try:\n",
    "  import chardet\n",
    "except:\n",
    "  !conda install --yes chardet\n",
    "  import chardet\n",
    "    \n",
    "!conda install --yes huggingface_hub\n",
    "!conda install --yes tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T17:27:14.084884Z",
     "iopub.status.busy": "2024-04-30T17:27:14.083809Z",
     "iopub.status.idle": "2024-04-30T17:27:14.313668Z",
     "shell.execute_reply": "2024-04-30T17:27:14.312932Z",
     "shell.execute_reply.started": "2024-04-30T17:27:14.084828Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T17:27:14.314865Z",
     "iopub.status.busy": "2024-04-30T17:27:14.314693Z",
     "iopub.status.idle": "2024-04-30T17:27:15.008034Z",
     "shell.execute_reply": "2024-04-30T17:27:15.006390Z",
     "shell.execute_reply.started": "2024-04-30T17:27:14.314848Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 30 17:27:14 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  On   | 00000000:04:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    25W / 250W |      0MiB / 19333MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T17:27:15.011052Z",
     "iopub.status.busy": "2024-04-30T17:27:15.010425Z",
     "iopub.status.idle": "2024-04-30T17:27:16.461250Z",
     "shell.execute_reply": "2024-04-30T17:27:16.460509Z",
     "shell.execute_reply.started": "2024-04-30T17:27:15.010992Z"
    },
    "id": "nAMzy_0FtaUZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os,sys\n",
    "import torch\n",
    "import torch as torch2\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "import time\n",
    "\n",
    "def S(any):\n",
    "  return \"{}\".format(any)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T17:27:16.462320Z",
     "iopub.status.busy": "2024-04-30T17:27:16.462046Z",
     "iopub.status.idle": "2024-04-30T17:27:16.466697Z",
     "shell.execute_reply": "2024-04-30T17:27:16.466188Z",
     "shell.execute_reply.started": "2024-04-30T17:27:16.462303Z"
    },
    "id": "04W4oeqScK0e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "#model_name = \"microsoft/phi-2\"\n",
    "#model_name = \"NousResearch/Llama-2-7b-chat-hf\" # The model that you want to train from the Hugging Face hub\n",
    "#model_name = \"openai-community/gpt2\"\n",
    "#model_name = \"Weblet/gpt2-medium-turbo1713756936088908_mlabonne-guanaco-llama2-1k_train\"\n",
    "#model_name = \"openai-community/gpt2-medium\"\n",
    "#model_name = \"microsoft/phi-1.5\"\n",
    "#model_name = \"Weblet/phi-1.5.2\"\n",
    "#model_name = \"microsoft/phi-1\"\n",
    "#model_name = \"/workspace/pring/llama-2-7b-chat-hf\"\n",
    "#model_name = \"Weblet/phi-1.5-turbo1713979458374441_mlabonne-guanaco-llama2-1k_train\"\n",
    "model_name = \"lvkaokao/llama2-7b-hf-chat-lora-v3\"\n",
    "model_splitname = model_name.split('/')\n",
    "model_subname = model_splitname[len(model_splitname)-1]\n",
    "dataset_name = \"mlabonne/guanaco-llama2-1k\" # The instruction dataset to use\n",
    "#dataset_name = \"mlabonne/orpo-dpo-mix-40k\"\n",
    "#dataset_name = \"cognitivecomputations/Code-290k-ShareGPT-Vicuna\"\n",
    "\n",
    "#new_model = \"Weblet/\" + model_subname + \"-turbo\"+ S(time.time()).replace('.','')# Fine-tuned model name\n",
    "new_model = \"/workspace/pring/\" + model_subname + S(time.time()).replace('.','')# Fine-tuned model name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T17:27:16.468934Z",
     "iopub.status.busy": "2024-04-30T17:27:16.468780Z",
     "iopub.status.idle": "2024-04-30T17:27:16.472268Z",
     "shell.execute_reply": "2024-04-30T17:27:16.471593Z",
     "shell.execute_reply.started": "2024-04-30T17:27:16.468919Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "#pipe = pipeline(\"text-generation\", model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T17:27:16.473662Z",
     "iopub.status.busy": "2024-04-30T17:27:16.473480Z",
     "iopub.status.idle": "2024-04-30T17:27:16.479117Z",
     "shell.execute_reply": "2024-04-30T17:27:16.478663Z",
     "shell.execute_reply.started": "2024-04-30T17:27:16.473644Z"
    },
    "id": "ib_We3NLtj2E",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "lora_r = 64 # LoRA attention dimension\n",
    "lora_alpha = 16 # Alpha parameter for LoRA scaling\n",
    "lora_dropout = 0.1 # Dropout probability for LoRA layers\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "use_4bit = True # Activate 4-bit precision base model loading\n",
    "bnb_4bit_compute_dtype = \"float16\" # Compute dtype for 4-bit base models\n",
    "bnb_4bit_quant_type = \"nf4\" # Quantization type (fp4 or nf4)\n",
    "use_nested_quant = True#False # Activate nested quantization for 4-bit base models (double quantization)\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "output_dir = \"./results\" # Output directory where the model predictions and checkpoints will be stored\n",
    "num_train_epochs = 1 # Number of training epochs\n",
    "fp16 = False # Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "bf16 = False\n",
    "per_device_train_batch_size = 1 # Batch size per GPU for training\n",
    "per_device_eval_batch_size = 1 # Batch size per GPU for evaluation\n",
    "gradient_accumulation_steps = 1 # Number of update steps to accumulate the gradients for\n",
    "gradient_checkpointing = True # Enable gradient checkpointing\n",
    "max_grad_norm = 0.3 # Maximum gradient normal (gradient clipping)\n",
    "learning_rate = 2e-4 # Initial learning rate (AdamW optimizer)\n",
    "weight_decay = 0.001 # Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "optim = \"paged_adamw_32bit\" # Optimizer to use\n",
    "lr_scheduler_type = \"cosine\" # Learning rate schedule\n",
    "max_steps = -1 # Number of training steps (overrides num_train_epochs)\n",
    "warmup_ratio = 0.03 # Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "group_by_length = False # Group sequences into batches with same length # Saves memory and speeds up training considerably\n",
    "save_steps = 0 # Save checkpoint every X updates steps\n",
    "logging_steps = 25 # Log every X updates steps\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "max_seq_length = 2048 # Maximum sequence length to use\n",
    "packing = False # Pack multiple short examples in the same input sequence to increase efficiency\n",
    "device_map = {\"\": 0} #  Load the entire model on the GPU 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "execution": {
     "iopub.execute_input": "2024-04-30T17:27:16.480428Z",
     "iopub.status.busy": "2024-04-30T17:27:16.480211Z",
     "iopub.status.idle": "2024-04-30T17:27:16.484838Z",
     "shell.execute_reply": "2024-04-30T17:27:16.484254Z",
     "shell.execute_reply.started": "2024-04-30T17:27:16.480411Z"
    },
    "id": "Q5EHNbUzCUNv",
    "outputId": "c534abfe-a809-44d6-f6df-ab422827058a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom datasets import ArrowBasedBuilder\\ndef globalThis():\\n  pass\\n\\ndef pss(*args, **kwargs):\\n  try:\\n    globalThis.ArrowBasedBuilder = ArrowBasedBuilder(*args, **kwargs)\\n  except:\\n    pass\\n  return globalThis.ArrowBasedBuilder\\n\\nArrowBasedBuilder._prepare_split_single = pss\\nprint(ArrowBasedBuilder._prepare_split_single)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from datasets import ArrowBasedBuilder\n",
    "def globalThis():\n",
    "  pass\n",
    "\n",
    "def pss(*args, **kwargs):\n",
    "  try:\n",
    "    globalThis.ArrowBasedBuilder = ArrowBasedBuilder(*args, **kwargs)\n",
    "  except:\n",
    "    pass\n",
    "  return globalThis.ArrowBasedBuilder\n",
    "\n",
    "ArrowBasedBuilder._prepare_split_single = pss\n",
    "print(ArrowBasedBuilder._prepare_split_single)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T17:27:16.485825Z",
     "iopub.status.busy": "2024-04-30T17:27:16.485590Z",
     "iopub.status.idle": "2024-04-30T17:27:19.004378Z",
     "shell.execute_reply": "2024-04-30T17:27:19.003665Z",
     "shell.execute_reply.started": "2024-04-30T17:27:16.485806Z"
    },
    "id": "VdcpiHdhgttR",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/datasets/load.py:2555: FutureWarning: 'ignore_verifications' was deprecated in favor of 'verification_mode' in version 2.9.1 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'verification_mode=no_checks' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load dataset (you can process it here)\n",
    "dataset_split = \"train\"\n",
    "#dataset_split = \"train[:1%]\"\n",
    "#dataset_split = \"train[5%:10%]\"\n",
    "#dataset_pre = load_dataset(dataset_name)\n",
    "try:\n",
    "  dataset_pre = load_dataset(dataset_name, split=dataset_split,encoding = 'UTF-8')\n",
    "except:\n",
    "  dataset_pre = load_dataset(dataset_name, split=dataset_split,ignore_verifications=True,verification_mode=\"no_checks\")\n",
    "dataset = dataset_pre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-30T17:27:19.005670Z",
     "iopub.status.busy": "2024-04-30T17:27:19.005491Z",
     "iopub.status.idle": "2024-04-30T17:27:19.011028Z",
     "shell.execute_reply": "2024-04-30T17:27:19.009835Z",
     "shell.execute_reply.started": "2024-04-30T17:27:19.005652Z"
    },
    "id": "ZGUkumL7ZKpw",
    "outputId": "9c7d42e0-4454-4d86-d44c-22437415f574",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if dataset_name == \"cognitivecomputations/Code-290k-ShareGPT-Vicuna\":\n",
    "  dataset_pre=dataset_pre.rename_column('conversations','text')\n",
    "  dataset_pre.set_format(type= None, format_kwargs= {}, columns= ['text'], output_all_columns= False)\n",
    "  def toPhi(data):\n",
    "    data['text']=\"<s>[INST]\"+data['text'][0]['value']+\"[/INST]\"+data['text'][1]['value']+\"</s>\"\n",
    "    return data\n",
    "  dataset = dataset_pre.map(toPhi)\n",
    "\n",
    "  print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-30T17:27:19.013026Z",
     "iopub.status.busy": "2024-04-30T17:27:19.012605Z",
     "iopub.status.idle": "2024-04-30T17:27:21.531996Z",
     "shell.execute_reply": "2024-04-30T17:27:21.531037Z",
     "shell.execute_reply.started": "2024-04-30T17:27:19.012986Z"
    },
    "id": "rW9PDhEllYvk",
    "outputId": "f03a21eb-4372-43d8-db4c-066ab1b7d266",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/runai-home/.cache/huggingface/token\n",
      "Login successful\n",
      "True\n",
      "cuda\n",
      "{'map_location': 'cpu', 'weights_only': False}\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to load weights from pytorch checkpoint file for '/home/runai-home/.cache/huggingface/hub/models--Weblet--Llama-2-7b-chat-hf-ct2-int8/snapshots/17a40e114974a27133a036c3b95c8f826996fd4b/pytorch_model.bin' at '/home/runai-home/.cache/huggingface/hub/models--Weblet--Llama-2-7b-chat-hf-ct2-int8/snapshots/17a40e114974a27133a036c3b95c8f826996fd4b/pytorch_model.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/modeling_utils.py:533\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, is_quantized)\u001b[0m\n\u001b[1;32m    532\u001b[0m     weights_only_kwarg \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_only\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m} \u001b[38;5;28;01mif\u001b[39;00m is_torch_greater_or_equal_than_1_13 \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mload(\n\u001b[1;32m    534\u001b[0m         checkpoint_file,\n\u001b[1;32m    535\u001b[0m         map_location\u001b[38;5;241m=\u001b[39mmap_location,\n\u001b[1;32m    536\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mweights_only_kwarg,\n\u001b[1;32m    537\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_args,\n\u001b[1;32m    538\u001b[0m     )\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[12], line 39\u001b[0m, in \u001b[0;36ml\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(kw)\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moldLoad(\u001b[38;5;241m*\u001b[39margs,\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/serialization.py:1028\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1028\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/serialization.py:1246\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load does not work with file-like objects that do not implement readinto on Python 3.8.0 and 3.8.1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1243\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived object of type \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(f)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m. Please update to Python 3.8.2 or newer to restore this \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1244\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctionality.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1246\u001b[0m magic_number \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mload(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic_number \u001b[38;5;241m!=\u001b[39m MAGIC_NUMBER:\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '\\x06'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/modeling_utils.py:542\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, is_quantized)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(checkpoint_file) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m7\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    544\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou seem to have cloned a repository without having git-lfs installed. Please install \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgit-lfs and run `git lfs install` followed by `git lfs pull` in the folder \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou cloned.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m         )\n",
      "File \u001b[0;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xef in position 33: invalid continuation byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 45\u001b[0m\n\u001b[1;32m     41\u001b[0m torch\u001b[38;5;241m.\u001b[39moldLoad \u001b[38;5;241m=\u001b[39m torch2\u001b[38;5;241m.\u001b[39mload\n\u001b[1;32m     42\u001b[0m torch\u001b[38;5;241m.\u001b[39mload \u001b[38;5;241m=\u001b[39m l\n\u001b[0;32m---> 45\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name,quantization_config\u001b[38;5;241m=\u001b[39mbnb_config,device_map\u001b[38;5;241m=\u001b[39mdevice_map)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#model = torch.load(\"https://huggingface.co/OpenNMT/Llama-2-7b-chat-hf-ct2-int8/resolve/main/model.bin\", map_location=\"cpu\")\u001b[39;00m\n\u001b[1;32m     48\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    564\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    565\u001b[0m     )\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/modeling_utils.py:3481\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_pt:\n\u001b[1;32m   3479\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sharded \u001b[38;5;129;01mand\u001b[39;00m state_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3480\u001b[0m         \u001b[38;5;66;03m# Time to load the checkpoint\u001b[39;00m\n\u001b[0;32m-> 3481\u001b[0m         state_dict \u001b[38;5;241m=\u001b[39m load_state_dict(resolved_archive_file)\n\u001b[1;32m   3483\u001b[0m     \u001b[38;5;66;03m# set dtype to instantiate the model under:\u001b[39;00m\n\u001b[1;32m   3484\u001b[0m     \u001b[38;5;66;03m# 1. If torch_dtype is not None, we use that dtype\u001b[39;00m\n\u001b[1;32m   3485\u001b[0m     \u001b[38;5;66;03m# 2. If torch_dtype is \"auto\", we auto-detect dtype from the loaded state_dict, by checking its first\u001b[39;00m\n\u001b[1;32m   3486\u001b[0m     \u001b[38;5;66;03m#    weights entry that is of a floating type - we assume all floating dtype weights are of the same dtype\u001b[39;00m\n\u001b[1;32m   3487\u001b[0m     \u001b[38;5;66;03m# we also may have config.torch_dtype available, but we won't rely on it till v5\u001b[39;00m\n\u001b[1;32m   3488\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/modeling_utils.py:554\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, is_quantized)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    550\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to locate the file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m which is necessary to load this pretrained \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    551\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel. Make sure you have saved the model properly.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    552\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    555\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load weights from pytorch checkpoint file for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    556\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mat \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m     )\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to load weights from pytorch checkpoint file for '/home/runai-home/.cache/huggingface/hub/models--Weblet--Llama-2-7b-chat-hf-ct2-int8/snapshots/17a40e114974a27133a036c3b95c8f826996fd4b/pytorch_model.bin' at '/home/runai-home/.cache/huggingface/hub/models--Weblet--Llama-2-7b-chat-hf-ct2-int8/snapshots/17a40e114974a27133a036c3b95c8f826996fd4b/pytorch_model.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True."
     ]
    }
   ],
   "source": [
    "\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "!huggingface-cli login --token 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\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "print(use_4bit)\n",
    "# Check GPU compatibility with bfloat16\n",
    "try:\n",
    "  if compute_dtype == torch.float16 and use_4bit:\n",
    "      major, _ = torch.cuda.get_device_capability()\n",
    "      if major >= 8:\n",
    "          print(\"=\" * 80)\n",
    "          print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "          print(\"=\" * 80)\n",
    "except:\n",
    "  device_map = \"cpu\"\n",
    "device_map=\"cuda\"\n",
    "\n",
    "print(device_map)\n",
    "# Load base model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def l(*args,**kwargs):\n",
    "  kw=kwargs\n",
    "  kw['weights_only']=False\n",
    "  print(kw)\n",
    "  return torch.oldLoad(*args,**kw)\n",
    "\n",
    "torch.oldLoad = torch2.load\n",
    "torch.load = l\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,quantization_config=bnb_config,device_map=device_map)\n",
    "#model = torch.load(\"https://huggingface.co/OpenNMT/Llama-2-7b-chat-hf-ct2-int8/resolve/main/model.bin\", map_location=\"cpu\")\n",
    "\n",
    "model.config.use_cache = True\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.status.busy": "2024-04-30T17:27:21.532646Z",
     "iopub.status.idle": "2024-04-30T17:27:21.532865Z",
     "shell.execute_reply": "2024-04-30T17:27:21.532775Z",
     "shell.execute_reply.started": "2024-04-30T17:27:21.532764Z"
    },
    "id": "XW94Kylolc_r",
    "outputId": "82e86c0a-43b0-4874-e2f8-255480db71b4"
   },
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "#logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"At what temperature does water boil?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "631e20ee4a0e4cfeae991cf1fea069a1",
      "8ee669f3cd504bb98648d0c9c598aa37",
      "e93819afef394d48ad16540e2cd14b2d",
      "ad14f7ec5d5d43cd8983c6f841f4092e",
      "f3ed89e0408c480fa75458b3aecc8248",
      "6d06606c1e3c4ccfa1335139fdcd43e0",
      "1024f48c3eb74964bb03b8fbb46ccd60",
      "5af754a056224ddb9496808fced259c0",
      "e4d9759289e541158a45e6967b0c4c58",
      "5dd1405ef5594f9f9daa5a9730611335",
      "aa0855f52c554bdebb1235a1dd7dbba6"
     ]
    },
    "execution": {
     "iopub.status.busy": "2024-04-30T17:27:21.533776Z",
     "iopub.status.idle": "2024-04-30T17:27:21.534001Z",
     "shell.execute_reply": "2024-04-30T17:27:21.533911Z",
     "shell.execute_reply.started": "2024-04-30T17:27:21.533900Z"
    },
    "id": "OJXpOgBFuSrc",
    "outputId": "178afa88-f096-4989-ac56-1fa294d73285",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "try:\n",
    "  trainer.train()\n",
    "except Exception as e: \n",
    "  new_model = new_model\n",
    "  print(e)\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-30T17:27:21.535060Z",
     "iopub.status.idle": "2024-04-30T17:27:21.535277Z",
     "shell.execute_reply": "2024-04-30T17:27:21.535168Z",
     "shell.execute_reply.started": "2024-04-30T17:27:21.535157Z"
    },
    "id": "crj9svNe4hU5"
   },
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir results/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "execution": {
     "iopub.status.busy": "2024-04-30T17:27:21.535998Z",
     "iopub.status.idle": "2024-04-30T17:27:21.536199Z",
     "shell.execute_reply": "2024-04-30T17:27:21.536105Z",
     "shell.execute_reply.started": "2024-04-30T17:27:21.536094Z"
    },
    "id": "frlSLPin4IJ4",
    "outputId": "976da46d-26f6-459c-bb8d-487a103c96a8"
   },
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"At what temperature does water boil?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-30T17:27:21.536857Z",
     "iopub.status.idle": "2024-04-30T17:27:21.537048Z",
     "shell.execute_reply": "2024-04-30T17:27:21.536961Z",
     "shell.execute_reply.started": "2024-04-30T17:27:21.536950Z"
    },
    "id": "mkQCviG0Zta-"
   },
   "outputs": [],
   "source": [
    "# Empty VRAM\n",
    "del model\n",
    "del pipe\n",
    "del trainer\n",
    "import gc\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "execution": {
     "iopub.status.busy": "2024-04-30T17:27:21.537940Z",
     "iopub.status.idle": "2024-04-30T17:27:21.538131Z",
     "shell.execute_reply": "2024-04-30T17:27:21.538044Z",
     "shell.execute_reply.started": "2024-04-30T17:27:21.538034Z"
    },
    "id": "QQn30cRtAZ-P",
    "outputId": "6d9eb287-d276-4da3-a469-6ff2af448481",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "print(type(new_model))\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-30T17:27:21.538983Z",
     "iopub.status.idle": "2024-04-30T17:27:21.539174Z",
     "shell.execute_reply": "2024-04-30T17:27:21.539088Z",
     "shell.execute_reply.started": "2024-04-30T17:27:21.539078Z"
    },
    "id": "zHLKfDGzsElK"
   },
   "outputs": [],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "prompt = \"At what temperature does water boil?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-30T17:27:21.539823Z",
     "iopub.status.idle": "2024-04-30T17:27:21.540014Z",
     "shell.execute_reply": "2024-04-30T17:27:21.539926Z",
     "shell.execute_reply.started": "2024-04-30T17:27:21.539916Z"
    },
    "id": "x-xPb-_qB0dz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "!huggingface-cli login --token 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\n",
    "\n",
    "model.push_to_hub((new_model+\"_\"+dataset_name.replace('/','-')+\"_\"+dataset_split.replace('[','').replace(']','').replace(':','').replace('%',''))[0:95], use_temp_dir=True)\n",
    "tokenizer.push_to_hub((new_model+\"_\"+dataset_name.replace('/','-')+\"_\"+dataset_split.replace('[','').replace(']','').replace(':','').replace('%',''))[0:95], use_temp_dir=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1024f48c3eb74964bb03b8fbb46ccd60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5af754a056224ddb9496808fced259c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5dd1405ef5594f9f9daa5a9730611335": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "631e20ee4a0e4cfeae991cf1fea069a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8ee669f3cd504bb98648d0c9c598aa37",
       "IPY_MODEL_e93819afef394d48ad16540e2cd14b2d",
       "IPY_MODEL_ad14f7ec5d5d43cd8983c6f841f4092e"
      ],
      "layout": "IPY_MODEL_f3ed89e0408c480fa75458b3aecc8248"
     }
    },
    "6d06606c1e3c4ccfa1335139fdcd43e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ee669f3cd504bb98648d0c9c598aa37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d06606c1e3c4ccfa1335139fdcd43e0",
      "placeholder": "​",
      "style": "IPY_MODEL_1024f48c3eb74964bb03b8fbb46ccd60",
      "value": "Map: 100%"
     }
    },
    "aa0855f52c554bdebb1235a1dd7dbba6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad14f7ec5d5d43cd8983c6f841f4092e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5dd1405ef5594f9f9daa5a9730611335",
      "placeholder": "​",
      "style": "IPY_MODEL_aa0855f52c554bdebb1235a1dd7dbba6",
      "value": " 28909/28909 [00:42&lt;00:00, 751.86 examples/s]"
     }
    },
    "e4d9759289e541158a45e6967b0c4c58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e93819afef394d48ad16540e2cd14b2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5af754a056224ddb9496808fced259c0",
      "max": 28909,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e4d9759289e541158a45e6967b0c4c58",
      "value": 28909
     }
    },
    "f3ed89e0408c480fa75458b3aecc8248": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
