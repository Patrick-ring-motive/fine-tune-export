{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "execution": {
     "iopub.execute_input": "2024-06-26T20:01:01.362178Z",
     "iopub.status.busy": "2024-06-26T20:01:01.362039Z",
     "iopub.status.idle": "2024-06-26T20:01:14.058411Z",
     "shell.execute_reply": "2024-06-26T20:01:14.057466Z",
     "shell.execute_reply.started": "2024-06-26T20:01:01.362164Z"
    },
    "id": "GLXwJqbjtPho",
    "outputId": "3a58991f-493c-430b-adb0-0a7c8170c72a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'pandas' from '/opt/conda/envs/pytorch/lib/python3.11/site-packages/pandas/__init__.py'>\n",
      "Successfully imported pandas.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'transformers' from '/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/__init__.py'>\n",
      "Successfully imported transformers.\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from metapython import *\n",
    "importLib(\"pandas\")\n",
    "#!rm -rf /workspace/pring/llama-2-7b-chat-hf1*\n",
    "#!pip install -U transformers\n",
    "#!conda install --yes tokenizers\n",
    "#!pip install --upgrade pip\n",
    "#!pip install tokenizers==0.15.2\n",
    "#!git clone https://github.com/NVIDIA/apex\n",
    "#!pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --config-settings \"--build-option=--cpp_ext\" --config-settings \"--build-option=--cuda_ext\" ./apex/\n",
    "importLib(\"transformers\")\n",
    "#!pip install transformers==4.13.0\n",
    "!conda install --yes pytorch==2.1.1 torchvision==0.16.1 torchaudio==2.1.1 pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "#!conda install --yes transformers[deepspeed]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-06-26T20:01:14.060393Z",
     "iopub.status.busy": "2024-06-26T20:01:14.059965Z",
     "iopub.status.idle": "2024-06-26T20:01:14.402318Z",
     "shell.execute_reply": "2024-06-26T20:01:14.401808Z",
     "shell.execute_reply.started": "2024-06-26T20:01:14.060372Z"
    },
    "id": "FmhXEL79Jp42",
    "outputId": "b9769b9c-5de6-427a-a797-837b40bf310d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'datasets' from '/opt/conda/envs/pytorch/lib/python3.11/site-packages/datasets/__init__.py'>\n",
      "Successfully imported datasets.\n",
      "<module 'chardet' from '/opt/conda/envs/pytorch/lib/python3.11/site-packages/chardet/__init__.py'>\n",
      "Successfully imported chardet.\n",
      "<module 'trl' from '/opt/conda/envs/pytorch/lib/python3.11/site-packages/trl/__init__.py'>\n",
      "Successfully imported trl.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'trl' from '/opt/conda/envs/pytorch/lib/python3.11/site-packages/trl/__init__.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "  import torch\n",
    "except:\n",
    "  !conda install --yes accelerate peft bitsandbytes transformers trl dataset torch==2.1.1\n",
    "  !conda install --yes torch==2.1.1\n",
    "import torch\n",
    "if (torch.cuda.is_available() == False ):\n",
    "  !conda install --yes torch==2.1.1\n",
    "  !conda install --yes accelerate peft bitsandbytes transformers trl dataset torch==2.1.1\n",
    "  import torch\n",
    "if (torch.cuda.is_available() == False ):\n",
    "    raise Exception(\"Reinstall pytorch\")\n",
    "    \n",
    "importLib(\"datasets\")\n",
    "importLib(\"chardet\")\n",
    "importLib(\"trl\")\n",
    "    \n",
    "#!conda install --yes huggingface_hub\n",
    "#!conda install --yes tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T20:01:14.403342Z",
     "iopub.status.busy": "2024-06-26T20:01:14.402998Z",
     "iopub.status.idle": "2024-06-26T20:01:14.527435Z",
     "shell.execute_reply": "2024-06-26T20:01:14.526852Z",
     "shell.execute_reply.started": "2024-06-26T20:01:14.403326Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T20:01:14.529069Z",
     "iopub.status.busy": "2024-06-26T20:01:14.528646Z",
     "iopub.status.idle": "2024-06-26T20:01:14.718783Z",
     "shell.execute_reply": "2024-06-26T20:01:14.718221Z",
     "shell.execute_reply.started": "2024-06-26T20:01:14.529052Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 26 20:01:14 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          Off |   00000000:9B:00.0 Off |                    0 |\n",
      "| N/A   40C    P0            119W /  700W |   15826MiB /  38146MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "magic('nvidia-smi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T20:01:14.719631Z",
     "iopub.status.busy": "2024-06-26T20:01:14.719475Z",
     "iopub.status.idle": "2024-06-26T20:01:14.722047Z",
     "shell.execute_reply": "2024-06-26T20:01:14.721575Z",
     "shell.execute_reply.started": "2024-06-26T20:01:14.719616Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T20:01:14.722745Z",
     "iopub.status.busy": "2024-06-26T20:01:14.722615Z",
     "iopub.status.idle": "2024-06-26T20:01:15.744994Z",
     "shell.execute_reply": "2024-06-26T20:01:15.744418Z",
     "shell.execute_reply.started": "2024-06-26T20:01:14.722733Z"
    },
    "id": "nAMzy_0FtaUZ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from datasets import load_dataset\n",
      "Successfully imported datasets.\n",
      "<function load_dataset at 0x7f7add84afc0>\n",
      "<module 'datasets' from '/opt/conda/envs/pytorch/lib/python3.11/site-packages/datasets/__init__.py'>\n",
      "Successfully imported datasets.\n",
      "<module 'trl' from '/opt/conda/envs/pytorch/lib/python3.11/site-packages/trl/__init__.py'>\n",
      "Successfully imported trl.\n",
      "from transformers import AutoModelForCausalLM\n",
      "Successfully imported transformers.\n",
      "<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>\n",
      "from transformers import AutoTokenizer\n",
      "Successfully imported transformers.\n",
      "<class 'transformers.models.auto.tokenization_auto.AutoTokenizer'>\n",
      "from transformers import BitsAndBytesConfig\n",
      "Successfully imported transformers.\n",
      "<class 'transformers.utils.quantization_config.BitsAndBytesConfig'>\n",
      "from transformers import HfArgumentParser\n",
      "Successfully imported transformers.\n",
      "<class 'transformers.hf_argparser.HfArgumentParser'>\n",
      "from transformers import TrainingArguments\n",
      "Successfully imported transformers.\n",
      "<class 'transformers.training_args.TrainingArguments'>\n",
      "from transformers import pipeline\n",
      "Successfully imported transformers.\n",
      "<function pipeline at 0x7f78d3b40680>\n",
      "from transformers import logging\n",
      "Successfully imported transformers.\n",
      "<module 'transformers.utils.logging' from '/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/utils/logging.py'>\n",
      "from peft import LoraConfig\n",
      "Successfully imported peft.\n",
      "<class 'peft.tuners.lora.config.LoraConfig'>\n",
      "from peft import PeftModel\n",
      "Successfully imported peft.\n",
      "<class 'peft.peft_model.PeftModel'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os,sys\n",
    "import torch\n",
    "fromImport(\"datasets\",[\"load_dataset\"])\n",
    "importLib(\"datasets\")\n",
    "importLib(\"trl\")\n",
    "fromImport(\"transformers\",[\"AutoModelForCausalLM\", \"AutoTokenizer\",\"BitsAndBytesConfig\",\"HfArgumentParser\",\"TrainingArguments\",\"pipeline\",\"logging\"])\n",
    "fromImport(\"peft\",[\"LoraConfig\", \"PeftModel\"])\n",
    "from trl import SFTTrainer\n",
    "import time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T20:01:15.745828Z",
     "iopub.status.busy": "2024-06-26T20:01:15.745683Z",
     "iopub.status.idle": "2024-06-26T20:01:15.749574Z",
     "shell.execute_reply": "2024-06-26T20:01:15.749120Z",
     "shell.execute_reply.started": "2024-06-26T20:01:15.745814Z"
    },
    "id": "04W4oeqScK0e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "#model_name = \"microsoft/phi-2\"\n",
    "#model_name = \"NousResearch/Llama-2-7b-chat-hf\" # The model that you want to train from the Hugging Face hub\n",
    "#model_name = \"openai-community/gpt2\"\n",
    "#model_name = \"Weblet/gpt2-medium-turbo1713756936088908_mlabonne-guanaco-llama2-1k_train\"\n",
    "#model_name = \"openai-community/gpt2-medium\"\n",
    "#model_name = \"microsoft/phi-1.5\"\n",
    "#model_name = \"Weblet/phi-1.5.2\"\n",
    "#model_name = \"microsoft/phi-1\"\n",
    "#model_name = \"/workspace/pring/llama-2-7b-chat-hf\"\n",
    "#model_name = \"Weblet/phi-1.5-turbo1713979458374441_mlabonne-guanaco-llama2-1k_train\"\n",
    "#model_name = \"lvkaokao/llama2-7b-hf-chat-lora-v3\"\n",
    "#model_name = \"Aspik101/llama-30b-instruct-2048-PL-lora\"\n",
    "#model_name = \"ytcheng/llama-3-8b-hf-sm-lora-merged\"\n",
    "#model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "model_name = \"Weblet/llama-2-usaaef2c\"\n",
    "\n",
    "model_splitname = model_name.split('/')\n",
    "model_subname = model_splitname[len(model_splitname)-1]\n",
    "#dataset_name = \"mlabonne/guanaco-llama2-1k\" # The instruction dataset to use\n",
    "#dataset_name = \"mlabonne/orpo-dpo-mix-40k\"\n",
    "#dataset_name = \"cognitivecomputations/Code-290k-ShareGPT-Vicuna\"\n",
    "dataset_name = \"/workspace/pring/fine-tune/cdataset\"\n",
    "\n",
    "new_model = \"Weblet/\" + model_subname + \"-turbo\"+ f\"{time.time()}\".replace('.','')# Fine-tuned model name\n",
    "#new_model = \"/workspace/pring/\" + model_subname + f\"{time.time()}\".replace('.','')# Fine-tuned model name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T20:01:15.750589Z",
     "iopub.status.busy": "2024-06-26T20:01:15.750436Z",
     "iopub.status.idle": "2024-06-26T20:01:15.754936Z",
     "shell.execute_reply": "2024-06-26T20:01:15.754433Z",
     "shell.execute_reply.started": "2024-06-26T20:01:15.750575Z"
    },
    "id": "ib_We3NLtj2E",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"backend:cudaMallocAsync\"\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "lora_r = 64 # LoRA attention dimension\n",
    "lora_alpha = 16 # Alpha parameter for LoRA scaling\n",
    "lora_dropout = 0.1 # Dropout probability for LoRA layers\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "use_4bit = True # Activate 4-bit precision base model loading\n",
    "bnb_4bit_compute_dtype = \"float16\" # Compute dtype for 4-bit base models\n",
    "bnb_4bit_quant_type = \"nf4\" # Quantization type (fp4 or nf4)\n",
    "use_nested_quant = False # Activate nested quantization for 4-bit base models (double quantization)\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "output_dir = \"./results\" # Output directory where the model predictions and checkpoints will be stored\n",
    "num_train_epochs = 1 # Number of training epochs\n",
    "fp16 = False # Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "bf16 = False\n",
    "per_device_train_batch_size = 1 # Batch size per GPU for training\n",
    "per_device_eval_batch_size = 1 # Batch size per GPU for evaluation\n",
    "gradient_accumulation_steps = 1 # Number of update steps to accumulate the gradients for\n",
    "gradient_checkpointing = True # Enable gradient checkpointing\n",
    "max_grad_norm = 0.3 # Maximum gradient normal (gradient clipping)\n",
    "learning_rate = 2e-4 # Initial learning rate (AdamW optimizer)\n",
    "weight_decay = 0.001 # Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "optim = \"paged_adamw_32bit\" #\"adamw_apex_fused\"# Optimizer to use\n",
    "lr_scheduler_type = \"cosine\" # Learning rate schedule\n",
    "max_steps = -1 # Number of training steps (overrides num_train_epochs)\n",
    "warmup_ratio = 0.03 # Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "group_by_length = False # Group sequences into batches with same length # Saves memory and speeds up training considerably\n",
    "save_steps = 0 # Save checkpoint every X updates steps\n",
    "logging_steps = 25 # Log every X updates steps\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "max_seq_length = 512 # Maximum sequence length to use\n",
    "packing = False # Pack multiple short examples in the same input sequence to increase efficiency\n",
    "device_map = {\"\": 0} #  Load the entire model on the GPU 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T20:01:15.755652Z",
     "iopub.status.busy": "2024-06-26T20:01:15.755517Z",
     "iopub.status.idle": "2024-06-26T20:01:15.810705Z",
     "shell.execute_reply": "2024-06-26T20:01:15.810235Z",
     "shell.execute_reply.started": "2024-06-26T20:01:15.755639Z"
    },
    "id": "VdcpiHdhgttR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load dataset (you can process it here)\n",
    "dataset_split = \"train\"\n",
    "#dataset_split = \"train[:1%]\"\n",
    "#dataset_split = \"train[5%:10%]\"\n",
    "#dataset_pre = load_dataset(dataset_name)\n",
    "try:\n",
    "    try:\n",
    "        try:\n",
    "            dataset_pre = load_dataset(dataset_name, split=dataset_split,encoding = 'UTF-8')\n",
    "        except:\n",
    "            dataset_pre = load_dataset(dataset_name, split=dataset_split,ignore_verifications=True,verification_mode=\"no_checks\")\n",
    "    except:\n",
    "        magic('%pip install -U datasets')\n",
    "        try:\n",
    "            dataset_pre = load_dataset(dataset_name, split=dataset_split,encoding = 'UTF-8')\n",
    "        except:\n",
    "            dataset_pre = load_dataset(dataset_name, split=dataset_split,ignore_verifications=True,verification_mode=\"no_checks\")\n",
    "except:\n",
    "    dataset_pre=pandas.read_csv(f\"{dataset_name}/dataset.xml\")\n",
    "dataset = dataset_pre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-06-26T20:01:15.812267Z",
     "iopub.status.busy": "2024-06-26T20:01:15.812116Z",
     "iopub.status.idle": "2024-06-26T20:01:15.815543Z",
     "shell.execute_reply": "2024-06-26T20:01:15.815104Z",
     "shell.execute_reply.started": "2024-06-26T20:01:15.812253Z"
    },
    "id": "ZGUkumL7ZKpw",
    "outputId": "9c7d42e0-4454-4d86-d44c-22437415f574",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if dataset_name == \"cognitivecomputations/Code-290k-ShareGPT-Vicuna\":\n",
    "  dataset_pre=dataset_pre.rename_column('conversations','text')\n",
    "  dataset_pre.set_format(type= None, format_kwargs= {}, columns= ['text'], output_all_columns= False)\n",
    "  def toPhi(data):\n",
    "    data['text']=\"<s>[INST]\"+data['text'][0]['value']+\"[/INST]\"+data['text'][1]['value']+\"</s>\"\n",
    "    return data\n",
    "  dataset = dataset_pre.map(toPhi)\n",
    "\n",
    "  print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T20:01:15.816259Z",
     "iopub.status.busy": "2024-06-26T20:01:15.816127Z",
     "iopub.status.idle": "2024-06-26T20:01:15.988000Z",
     "shell.execute_reply": "2024-06-26T20:01:15.987537Z",
     "shell.execute_reply.started": "2024-06-26T20:01:15.816246Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Empty VRAM\n",
    "import gc\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-06-26T20:01:15.988812Z",
     "iopub.status.busy": "2024-06-26T20:01:15.988671Z",
     "iopub.status.idle": "2024-06-26T20:06:44.288138Z",
     "shell.execute_reply": "2024-06-26T20:06:44.287584Z",
     "shell.execute_reply.started": "2024-06-26T20:01:15.988798Z"
    },
    "id": "rW9PDhEllYvk",
    "outputId": "f03a21eb-4372-43d8-db4c-066ab1b7d266",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'sentencepiece' from '/opt/conda/envs/pytorch/lib/python3.11/site-packages/sentencepiece/__init__.py'>\n",
      "Successfully imported sentencepiece.\n",
      "Requirement already satisfied: flash-attn in /opt/conda/envs/pytorch/lib/python3.11/site-packages (2.5.9.post1)\n",
      "Requirement already satisfied: torch in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from flash-attn) (2.1.1)\n",
      "Requirement already satisfied: einops in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from flash-attn) (0.8.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from torch->flash-attn) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from torch->flash-attn) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from torch->flash-attn) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from torch->flash-attn) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from torch->flash-attn) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from torch->flash-attn) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from jinja2->torch->flash-attn) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from sympy->torch->flash-attn) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: peft in /opt/conda/envs/pytorch/lib/python3.11/site-packages (0.11.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from peft) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from peft) (6.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from peft) (2.1.1)\n",
      "Requirement already satisfied: transformers in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from peft) (4.37.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from peft) (4.66.4)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from peft) (0.31.0)\n",
      "Requirement already satisfied: safetensors in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from peft) (0.4.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from peft) (0.23.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (2024.3.1)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from transformers->peft) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from transformers->peft) (0.15.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.6.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "<module 'tensorboardX' from '/opt/conda/envs/pytorch/lib/python3.11/site-packages/tensorboardX/__init__.py'>\n",
      "Successfully imported tensorboardX.\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/runai-home/.cache/huggingface/token\n",
      "Login successful\n",
      "True\n",
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n",
      "{'': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Downloading shards: 100%|██████████| 3/3 [05:18<00:00, 106.10s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "importLib('sentencepiece')\n",
    "magic('%pip install -U flash-attn');\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "magic('%pip install -U peft')\n",
    "importLib('tensorboardX')\n",
    "\n",
    "!huggingface-cli login --token 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\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "try:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=use_4bit,\n",
    "        bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=use_nested_quant,\n",
    "    )\n",
    "except:\n",
    "    magic('%pip install -U bitsandbytes');\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=use_4bit,\n",
    "        bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=use_nested_quant,\n",
    "    )\n",
    "print(use_4bit)\n",
    "# Check GPU compatibility with bfloat16\n",
    "try:\n",
    "  if compute_dtype == torch.float16 and use_4bit:\n",
    "      major, _ = torch.cuda.get_device_capability()\n",
    "      if major >= 8:\n",
    "          print(\"=\" * 80)\n",
    "          print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "          print(\"=\" * 80)\n",
    "except:\n",
    "  device_map = \"cpu\"\n",
    "\n",
    "print(device_map)\n",
    "\n",
    "model=AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T20:06:44.289169Z",
     "iopub.status.busy": "2024-06-26T20:06:44.288879Z",
     "iopub.status.idle": "2024-06-26T20:06:44.291761Z",
     "shell.execute_reply": "2024-06-26T20:06:44.291254Z",
     "shell.execute_reply.started": "2024-06-26T20:06:44.289152Z"
    },
    "id": "crj9svNe4hU5"
   },
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir results/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "execution": {
     "iopub.execute_input": "2024-06-26T20:09:14.533292Z",
     "iopub.status.busy": "2024-06-26T20:09:14.532941Z",
     "iopub.status.idle": "2024-06-26T20:09:22.908268Z",
     "shell.execute_reply": "2024-06-26T20:09:22.907595Z",
     "shell.execute_reply.started": "2024-06-26T20:09:14.533274Z"
    },
    "id": "frlSLPin4IJ4",
    "outputId": "976da46d-26f6-459c-bb8d-487a103c96a8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Tell me about how USAA members might save for retirement [/INST] How USAA Members Can Save for Retirement is an Educational Article that provides information on how USAA members can save for retirement., including the importance of retirement savings, common savings options, and strategies for building a retirement nest egg. It also includes a calculator to help readers determine how much they need to save for retirement.\n",
      "\n",
      "The article emphasizes the importance of saving for retirement and encourages readers to take advantage of tax-advantaged retirement accounts, such as a 401(k) or an IRA. It also provides tips on how to make the most of these accounts, including contributing as much as possible, taking advantage of catch-up contributions, and avoiding withdrawals before age 59 1/2.\n",
      "\n",
      "The article also touches on other retirement savings options, such as annuities and real estate, and provides guidance on how to balance these options with other financial goals, such as saving for emergencies or paying off debt.\n",
      "\n",
      "The article is intended for USAA members who are interested in saving for retirement, but may not have a lot of experience with retirement savings. It is designed to provide practical advice and guidance on how to get started with retirement savings, as well as strategies for building a successful retirement nest egg.\n",
      "\n",
      "The article is part of a larger section on USAA's website that provides information and resources on retirement savings and investing. It is intended to be a comprehensive resource for USAA members who are looking to save for retirement and make informed decisions about their retirement savings.\n",
      "\n",
      "The article is well-written and easy to understand, with clear and concise language and practical tips and strategies for saving for retirement. It is also visually appealing, with a variety of graphics and charts that help to illustrate key points.\n",
      "\n",
      "Overall, How USAA Members Can Save for Retirement is an informative and practical article that provides valuable insights and guidance on how to save for retirement. It is a valuable resource for USAA members who are looking to build a successful retirement nest egg and achieve their retirement goals.\n",
      "\n",
      "Read More\n",
      "How USAA Members Can Save for Retirement\n",
      "if (document.readyState === 'loading') {\n"
     ]
    }
   ],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"Tell me about how USAA members might save for retirement\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=max_seq_length)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1024f48c3eb74964bb03b8fbb46ccd60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5af754a056224ddb9496808fced259c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5dd1405ef5594f9f9daa5a9730611335": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "631e20ee4a0e4cfeae991cf1fea069a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8ee669f3cd504bb98648d0c9c598aa37",
       "IPY_MODEL_e93819afef394d48ad16540e2cd14b2d",
       "IPY_MODEL_ad14f7ec5d5d43cd8983c6f841f4092e"
      ],
      "layout": "IPY_MODEL_f3ed89e0408c480fa75458b3aecc8248"
     }
    },
    "6d06606c1e3c4ccfa1335139fdcd43e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ee669f3cd504bb98648d0c9c598aa37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d06606c1e3c4ccfa1335139fdcd43e0",
      "placeholder": "​",
      "style": "IPY_MODEL_1024f48c3eb74964bb03b8fbb46ccd60",
      "value": "Map: 100%"
     }
    },
    "aa0855f52c554bdebb1235a1dd7dbba6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad14f7ec5d5d43cd8983c6f841f4092e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5dd1405ef5594f9f9daa5a9730611335",
      "placeholder": "​",
      "style": "IPY_MODEL_aa0855f52c554bdebb1235a1dd7dbba6",
      "value": " 28909/28909 [00:42&lt;00:00, 751.86 examples/s]"
     }
    },
    "e4d9759289e541158a45e6967b0c4c58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e93819afef394d48ad16540e2cd14b2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5af754a056224ddb9496808fced259c0",
      "max": 28909,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e4d9759289e541158a45e6967b0c4c58",
      "value": 28909
     }
    },
    "f3ed89e0408c480fa75458b3aecc8248": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
