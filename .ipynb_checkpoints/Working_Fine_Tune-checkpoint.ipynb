{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "execution": {
     "iopub.execute_input": "2024-06-26T17:04:38.045141Z",
     "iopub.status.busy": "2024-06-26T17:04:38.044789Z",
     "iopub.status.idle": "2024-06-26T17:04:51.429249Z",
     "shell.execute_reply": "2024-06-26T17:04:51.428458Z",
     "shell.execute_reply.started": "2024-06-26T17:04:38.045123Z"
    },
    "id": "GLXwJqbjtPho",
    "outputId": "3a58991f-493c-430b-adb0-0a7c8170c72a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'pandas' from '/opt/conda/envs/pytorch/lib/python3.11/site-packages/pandas/__init__.py'>\n",
      "Successfully imported pandas.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'transformers' from '/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/__init__.py'>\n",
      "Successfully imported transformers.\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from metapython import *\n",
    "importLib(\"pandas\")\n",
    "#!rm -rf /workspace/pring/llama-2-7b-chat-hf1*\n",
    "#!pip install -U transformers\n",
    "#!conda install --yes tokenizers\n",
    "#!pip install --upgrade pip\n",
    "#!pip install tokenizers==0.15.2\n",
    "#!git clone https://github.com/NVIDIA/apex\n",
    "#!pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --config-settings \"--build-option=--cpp_ext\" --config-settings \"--build-option=--cuda_ext\" ./apex/\n",
    "importLib(\"transformers\")\n",
    "#!pip install transformers==4.13.0\n",
    "!conda install --yes pytorch==2.1.1 torchvision==0.16.1 torchaudio==2.1.1 pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "#!conda install --yes transformers[deepspeed]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-06-26T17:04:51.431245Z",
     "iopub.status.busy": "2024-06-26T17:04:51.430815Z",
     "iopub.status.idle": "2024-06-26T17:04:51.727733Z",
     "shell.execute_reply": "2024-06-26T17:04:51.727247Z",
     "shell.execute_reply.started": "2024-06-26T17:04:51.431224Z"
    },
    "id": "FmhXEL79Jp42",
    "outputId": "b9769b9c-5de6-427a-a797-837b40bf310d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'datasets' from '/opt/conda/envs/pytorch/lib/python3.11/site-packages/datasets/__init__.py'>\n",
      "Successfully imported datasets.\n",
      "<module 'chardet' from '/opt/conda/envs/pytorch/lib/python3.11/site-packages/chardet/__init__.py'>\n",
      "Successfully imported chardet.\n",
      "<module 'trl' from '/opt/conda/envs/pytorch/lib/python3.11/site-packages/trl/__init__.py'>\n",
      "Successfully imported trl.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'trl' from '/opt/conda/envs/pytorch/lib/python3.11/site-packages/trl/__init__.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "  import torch\n",
    "except:\n",
    "  !conda install --yes accelerate peft bitsandbytes transformers trl dataset torch==2.1.1\n",
    "  !conda install --yes torch==2.1.1\n",
    "import torch\n",
    "if (torch.cuda.is_available() == False ):\n",
    "  !conda install --yes torch==2.1.1\n",
    "  !conda install --yes accelerate peft bitsandbytes transformers trl dataset torch==2.1.1\n",
    "  import torch\n",
    "if (torch.cuda.is_available() == False ):\n",
    "    raise Exception(\"Reinstall pytorch\")\n",
    "    \n",
    "importLib(\"datasets\")\n",
    "importLib(\"chardet\")\n",
    "importLib(\"trl\")\n",
    "    \n",
    "#!conda install --yes huggingface_hub\n",
    "#!conda install --yes tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T17:04:51.728695Z",
     "iopub.status.busy": "2024-06-26T17:04:51.728423Z",
     "iopub.status.idle": "2024-06-26T17:04:51.855287Z",
     "shell.execute_reply": "2024-06-26T17:04:51.854843Z",
     "shell.execute_reply.started": "2024-06-26T17:04:51.728680Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T17:04:51.856591Z",
     "iopub.status.busy": "2024-06-26T17:04:51.856441Z",
     "iopub.status.idle": "2024-06-26T17:04:52.058991Z",
     "shell.execute_reply": "2024-06-26T17:04:52.058407Z",
     "shell.execute_reply.started": "2024-06-26T17:04:51.856577Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 26 17:04:51 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          Off |   00000000:9B:00.0 Off |                    0 |\n",
      "| N/A   40C    P0            119W /  700W |    8756MiB /  38146MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "magic('nvidia-smi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T17:04:52.059861Z",
     "iopub.status.busy": "2024-06-26T17:04:52.059708Z",
     "iopub.status.idle": "2024-06-26T17:04:52.062367Z",
     "shell.execute_reply": "2024-06-26T17:04:52.061878Z",
     "shell.execute_reply.started": "2024-06-26T17:04:52.059846Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T17:04:52.063009Z",
     "iopub.status.busy": "2024-06-26T17:04:52.062886Z",
     "iopub.status.idle": "2024-06-26T17:04:53.094339Z",
     "shell.execute_reply": "2024-06-26T17:04:53.093748Z",
     "shell.execute_reply.started": "2024-06-26T17:04:52.062997Z"
    },
    "id": "nAMzy_0FtaUZ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from datasets import load_dataset\n",
      "Successfully imported datasets.\n",
      "<function load_dataset at 0x7ff29e6f6f20>\n",
      "<module 'datasets' from '/opt/conda/envs/pytorch/lib/python3.11/site-packages/datasets/__init__.py'>\n",
      "Successfully imported datasets.\n",
      "<module 'trl' from '/opt/conda/envs/pytorch/lib/python3.11/site-packages/trl/__init__.py'>\n",
      "Successfully imported trl.\n",
      "from transformers import AutoModelForCausalLM\n",
      "Successfully imported transformers.\n",
      "<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>\n",
      "from transformers import AutoTokenizer\n",
      "Successfully imported transformers.\n",
      "<class 'transformers.models.auto.tokenization_auto.AutoTokenizer'>\n",
      "from transformers import BitsAndBytesConfig\n",
      "Successfully imported transformers.\n",
      "<class 'transformers.utils.quantization_config.BitsAndBytesConfig'>\n",
      "from transformers import HfArgumentParser\n",
      "Successfully imported transformers.\n",
      "<class 'transformers.hf_argparser.HfArgumentParser'>\n",
      "from transformers import TrainingArguments\n",
      "Successfully imported transformers.\n",
      "<class 'transformers.training_args.TrainingArguments'>\n",
      "from transformers import pipeline\n",
      "Successfully imported transformers.\n",
      "<function pipeline at 0x7ff29ca28680>\n",
      "from transformers import logging\n",
      "Successfully imported transformers.\n",
      "<module 'transformers.utils.logging' from '/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/utils/logging.py'>\n",
      "from peft import LoraConfig\n",
      "Successfully imported peft.\n",
      "<class 'peft.tuners.lora.config.LoraConfig'>\n",
      "from peft import PeftModel\n",
      "Successfully imported peft.\n",
      "<class 'peft.peft_model.PeftModel'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os,sys\n",
    "import torch\n",
    "fromImport(\"datasets\",[\"load_dataset\"])\n",
    "importLib(\"datasets\")\n",
    "importLib(\"trl\")\n",
    "fromImport(\"transformers\",[\"AutoModelForCausalLM\", \"AutoTokenizer\",\"BitsAndBytesConfig\",\"HfArgumentParser\",\"TrainingArguments\",\"pipeline\",\"logging\"])\n",
    "fromImport(\"peft\",[\"LoraConfig\", \"PeftModel\"])\n",
    "from trl import SFTTrainer\n",
    "import time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T17:04:53.095197Z",
     "iopub.status.busy": "2024-06-26T17:04:53.095042Z",
     "iopub.status.idle": "2024-06-26T17:04:53.098957Z",
     "shell.execute_reply": "2024-06-26T17:04:53.098428Z",
     "shell.execute_reply.started": "2024-06-26T17:04:53.095181Z"
    },
    "id": "04W4oeqScK0e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "#model_name = \"microsoft/phi-2\"\n",
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\" # The model that you want to train from the Hugging Face hub\n",
    "#model_name = \"openai-community/gpt2\"\n",
    "#model_name = \"Weblet/gpt2-medium-turbo1713756936088908_mlabonne-guanaco-llama2-1k_train\"\n",
    "#model_name = \"openai-community/gpt2-medium\"\n",
    "#model_name = \"microsoft/phi-1.5\"\n",
    "#model_name = \"Weblet/phi-1.5.2\"\n",
    "#model_name = \"microsoft/phi-1\"\n",
    "#model_name = \"/workspace/pring/llama-2-7b-chat-hf\"\n",
    "#model_name = \"Weblet/phi-1.5-turbo1713979458374441_mlabonne-guanaco-llama2-1k_train\"\n",
    "#model_name = \"lvkaokao/llama2-7b-hf-chat-lora-v3\"\n",
    "#model_name = \"Aspik101/llama-30b-instruct-2048-PL-lora\"\n",
    "#model_name = \"ytcheng/llama-3-8b-hf-sm-lora-merged\"\n",
    "#model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "model_splitname = model_name.split('/')\n",
    "model_subname = model_splitname[len(model_splitname)-1]\n",
    "#dataset_name = \"mlabonne/guanaco-llama2-1k\" # The instruction dataset to use\n",
    "#dataset_name = \"mlabonne/orpo-dpo-mix-40k\"\n",
    "#dataset_name = \"cognitivecomputations/Code-290k-ShareGPT-Vicuna\"\n",
    "dataset_name = \"/workspace/pring/fine-tune/dataset\"\n",
    "\n",
    "new_model = \"Weblet/\" + model_subname + \"-turbo\"+ f\"{time.time()}\".replace('.','')# Fine-tuned model name\n",
    "#new_model = \"/workspace/pring/\" + model_subname + f\"{time.time()}\".replace('.','')# Fine-tuned model name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T17:04:53.099677Z",
     "iopub.status.busy": "2024-06-26T17:04:53.099535Z",
     "iopub.status.idle": "2024-06-26T17:04:53.104213Z",
     "shell.execute_reply": "2024-06-26T17:04:53.103714Z",
     "shell.execute_reply.started": "2024-06-26T17:04:53.099663Z"
    },
    "id": "ib_We3NLtj2E",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"backend:cudaMallocAsync\"\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "lora_r = 64 # LoRA attention dimension\n",
    "lora_alpha = 16 # Alpha parameter for LoRA scaling\n",
    "lora_dropout = 0.1 # Dropout probability for LoRA layers\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "use_4bit = True # Activate 4-bit precision base model loading\n",
    "bnb_4bit_compute_dtype = \"float16\" # Compute dtype for 4-bit base models\n",
    "bnb_4bit_quant_type = \"nf4\" # Quantization type (fp4 or nf4)\n",
    "use_nested_quant = False # Activate nested quantization for 4-bit base models (double quantization)\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "output_dir = \"./results\" # Output directory where the model predictions and checkpoints will be stored\n",
    "num_train_epochs = 1 # Number of training epochs\n",
    "fp16 = False # Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "bf16 = False\n",
    "per_device_train_batch_size = 1 # Batch size per GPU for training\n",
    "per_device_eval_batch_size = 1 # Batch size per GPU for evaluation\n",
    "gradient_accumulation_steps = 1 # Number of update steps to accumulate the gradients for\n",
    "gradient_checkpointing = True # Enable gradient checkpointing\n",
    "max_grad_norm = 0.3 # Maximum gradient normal (gradient clipping)\n",
    "learning_rate = 2e-4 # Initial learning rate (AdamW optimizer)\n",
    "weight_decay = 0.001 # Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "optim = \"paged_adamw_32bit\" #\"adamw_apex_fused\"# Optimizer to use\n",
    "lr_scheduler_type = \"cosine\" # Learning rate schedule\n",
    "max_steps = -1 # Number of training steps (overrides num_train_epochs)\n",
    "warmup_ratio = 0.03 # Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "group_by_length = False # Group sequences into batches with same length # Saves memory and speeds up training considerably\n",
    "save_steps = 0 # Save checkpoint every X updates steps\n",
    "logging_steps = 25 # Log every X updates steps\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "max_seq_length = 512 # Maximum sequence length to use\n",
    "packing = False # Pack multiple short examples in the same input sequence to increase efficiency\n",
    "device_map = {\"\": 0} #  Load the entire model on the GPU 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "execution": {
     "iopub.execute_input": "2024-06-26T17:04:53.104887Z",
     "iopub.status.busy": "2024-06-26T17:04:53.104755Z",
     "iopub.status.idle": "2024-06-26T17:04:53.108095Z",
     "shell.execute_reply": "2024-06-26T17:04:53.107699Z",
     "shell.execute_reply.started": "2024-06-26T17:04:53.104875Z"
    },
    "id": "Q5EHNbUzCUNv",
    "outputId": "c534abfe-a809-44d6-f6df-ab422827058a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom datasets import ArrowBasedBuilder\\ndef globalThis():\\n  pass\\n\\ndef pss(*args, **kwargs):\\n  try:\\n    globalThis.ArrowBasedBuilder = ArrowBasedBuilder(*args, **kwargs)\\n  except:\\n    pass\\n  return globalThis.ArrowBasedBuilder\\n\\nArrowBasedBuilder._prepare_split_single = pss\\nprint(ArrowBasedBuilder._prepare_split_single)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from datasets import ArrowBasedBuilder\n",
    "def globalThis():\n",
    "  pass\n",
    "\n",
    "def pss(*args, **kwargs):\n",
    "  try:\n",
    "    globalThis.ArrowBasedBuilder = ArrowBasedBuilder(*args, **kwargs)\n",
    "  except:\n",
    "    pass\n",
    "  return globalThis.ArrowBasedBuilder\n",
    "\n",
    "ArrowBasedBuilder._prepare_split_single = pss\n",
    "print(ArrowBasedBuilder._prepare_split_single)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T17:04:53.109797Z",
     "iopub.status.busy": "2024-06-26T17:04:53.109650Z",
     "iopub.status.idle": "2024-06-26T17:04:53.186993Z",
     "shell.execute_reply": "2024-06-26T17:04:53.186513Z",
     "shell.execute_reply.started": "2024-06-26T17:04:53.109784Z"
    },
    "id": "VdcpiHdhgttR",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 139 examples [00:00, 16068.80 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load dataset (you can process it here)\n",
    "dataset_split = \"train\"\n",
    "#dataset_split = \"train[:1%]\"\n",
    "#dataset_split = \"train[5%:10%]\"\n",
    "#dataset_pre = load_dataset(dataset_name)\n",
    "try:\n",
    "    try:\n",
    "        try:\n",
    "            dataset_pre = load_dataset(dataset_name, split=dataset_split,encoding = 'UTF-8')\n",
    "        except:\n",
    "            dataset_pre = load_dataset(dataset_name, split=dataset_split,ignore_verifications=True,verification_mode=\"no_checks\")\n",
    "    except:\n",
    "        magic('%pip install -U datasets')\n",
    "        try:\n",
    "            dataset_pre = load_dataset(dataset_name, split=dataset_split,encoding = 'UTF-8')\n",
    "        except:\n",
    "            dataset_pre = load_dataset(dataset_name, split=dataset_split,ignore_verifications=True,verification_mode=\"no_checks\")\n",
    "except:\n",
    "    dataset_pre=pandas.read_csv(f\"{dataset_name}/dataset.xml\")\n",
    "dataset = dataset_pre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-06-26T17:04:53.187727Z",
     "iopub.status.busy": "2024-06-26T17:04:53.187588Z",
     "iopub.status.idle": "2024-06-26T17:04:53.191200Z",
     "shell.execute_reply": "2024-06-26T17:04:53.190668Z",
     "shell.execute_reply.started": "2024-06-26T17:04:53.187713Z"
    },
    "id": "ZGUkumL7ZKpw",
    "outputId": "9c7d42e0-4454-4d86-d44c-22437415f574",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if dataset_name == \"cognitivecomputations/Code-290k-ShareGPT-Vicuna\":\n",
    "  dataset_pre=dataset_pre.rename_column('conversations','text')\n",
    "  dataset_pre.set_format(type= None, format_kwargs= {}, columns= ['text'], output_all_columns= False)\n",
    "  def toPhi(data):\n",
    "    data['text']=\"<s>[INST]\"+data['text'][0]['value']+\"[/INST]\"+data['text'][1]['value']+\"</s>\"\n",
    "    return data\n",
    "  dataset = dataset_pre.map(toPhi)\n",
    "\n",
    "  print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-06-26T17:04:53.192114Z",
     "iopub.status.busy": "2024-06-26T17:04:53.191932Z",
     "iopub.status.idle": "2024-06-26T17:05:06.887886Z",
     "shell.execute_reply": "2024-06-26T17:05:06.887272Z",
     "shell.execute_reply.started": "2024-06-26T17:04:53.192100Z"
    },
    "id": "rW9PDhEllYvk",
    "outputId": "f03a21eb-4372-43d8-db4c-066ab1b7d266",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'sentencepiece' from '/opt/conda/envs/pytorch/lib/python3.11/site-packages/sentencepiece/__init__.py'>\n",
      "Successfully imported sentencepiece.\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/runai-home/.cache/huggingface/token\n",
      "Login successful\n",
      "True\n",
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n",
      "{'': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.66s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "importLib('sentencepiece')\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "!huggingface-cli login --token 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\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "try:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=use_4bit,\n",
    "        bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=use_nested_quant,\n",
    "    )\n",
    "except:\n",
    "    magic('%pip install -U bitsandbytes');\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=use_4bit,\n",
    "        bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=use_nested_quant,\n",
    "    )\n",
    "print(use_4bit)\n",
    "# Check GPU compatibility with bfloat16\n",
    "try:\n",
    "  if compute_dtype == torch.float16 and use_4bit:\n",
    "      major, _ = torch.cuda.get_device_capability()\n",
    "      if major >= 8:\n",
    "          print(\"=\" * 80)\n",
    "          print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "          print(\"=\" * 80)\n",
    "except:\n",
    "  device_map = \"cpu\"\n",
    "\n",
    "print(device_map)\n",
    "# Load base model\n",
    "try:\n",
    "  model = AutoModelForCausalLM.from_pretrained(model_name,quantization_config=bnb_config,device_map=device_map,trust_remote_code=True)\n",
    "except:\n",
    "  model = AutoModelForCausalLM.from_pretrained(model_name,device_map=device_map,trust_remote_code=True)\n",
    "model.config.use_cache = True\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-06-26T17:05:06.888869Z",
     "iopub.status.busy": "2024-06-26T17:05:06.888714Z",
     "iopub.status.idle": "2024-06-26T17:05:06.892607Z",
     "shell.execute_reply": "2024-06-26T17:05:06.892108Z",
     "shell.execute_reply.started": "2024-06-26T17:05:06.888853Z"
    },
    "id": "XW94Kylolc_r",
    "outputId": "82e86c0a-43b0-4874-e2f8-255480db71b4",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prompt = \"At what temperature does water boil?\"\\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=max_seq_length)\\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\\nprint(result[0][\\'generated_text\\'])'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ignore warnings\n",
    "#logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "\"\"\"prompt = \"At what temperature does water boil?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=max_seq_length)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "631e20ee4a0e4cfeae991cf1fea069a1",
      "8ee669f3cd504bb98648d0c9c598aa37",
      "e93819afef394d48ad16540e2cd14b2d",
      "ad14f7ec5d5d43cd8983c6f841f4092e",
      "f3ed89e0408c480fa75458b3aecc8248",
      "6d06606c1e3c4ccfa1335139fdcd43e0",
      "1024f48c3eb74964bb03b8fbb46ccd60",
      "5af754a056224ddb9496808fced259c0",
      "e4d9759289e541158a45e6967b0c4c58",
      "5dd1405ef5594f9f9daa5a9730611335",
      "aa0855f52c554bdebb1235a1dd7dbba6"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-06-26T17:05:06.893372Z",
     "iopub.status.busy": "2024-06-26T17:05:06.893233Z",
     "iopub.status.idle": "2024-06-26T17:05:36.487449Z",
     "shell.execute_reply": "2024-06-26T17:05:36.486473Z",
     "shell.execute_reply.started": "2024-06-26T17:05:06.893359Z"
    },
    "id": "OJXpOgBFuSrc",
    "outputId": "178afa88-f096-4989-ac56-1fa294d73285",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in /opt/conda/envs/pytorch/lib/python3.11/site-packages (0.11.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from peft) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from peft) (6.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from peft) (2.1.1)\n",
      "Requirement already satisfied: transformers in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from peft) (4.37.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from peft) (4.66.4)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from peft) (0.31.0)\n",
      "Requirement already satisfied: safetensors in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from peft) (0.4.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from peft) (0.23.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (2024.3.1)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from transformers->peft) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from transformers->peft) (0.15.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.6.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/pytorch/lib/python3.11/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "<module 'tensorboardX' from '/opt/conda/envs/pytorch/lib/python3.11/site-packages/tensorboardX/__init__.py'>\n",
      "Successfully imported tensorboardX.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/training_args.py:1741: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/training_args.py:1741: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 139/139 [00:00<00:00, 1210.91 examples/s]\n",
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [139/139 00:23, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.105700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.107600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.839100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.871100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Load LoRA configuration\n",
    "magic('%pip install -U peft')\n",
    "importLib('tensorboardX')\n",
    "\n",
    "target_modules=[\n",
    "\"q_proj\",\n",
    "\"k_proj\",\n",
    "\"v_proj\",\n",
    "\"o_proj\",\n",
    "\"gate_proj\",\n",
    "\"up_proj\",\n",
    "\"down_proj\",\n",
    "\"lm_head\",\n",
    "]\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=target_modules\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "#try:\n",
    "trainer.train()\n",
    "#except Exception as e: \n",
    "#  new_model = new_model\n",
    "#  print(e)\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T17:05:36.489239Z",
     "iopub.status.busy": "2024-06-26T17:05:36.488874Z",
     "iopub.status.idle": "2024-06-26T17:05:36.492409Z",
     "shell.execute_reply": "2024-06-26T17:05:36.491940Z",
     "shell.execute_reply.started": "2024-06-26T17:05:36.489222Z"
    },
    "id": "crj9svNe4hU5"
   },
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir results/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "execution": {
     "iopub.execute_input": "2024-06-26T17:05:36.493158Z",
     "iopub.status.busy": "2024-06-26T17:05:36.493027Z",
     "iopub.status.idle": "2024-06-26T17:05:39.222703Z",
     "shell.execute_reply": "2024-06-26T17:05:39.221852Z",
     "shell.execute_reply.started": "2024-06-26T17:05:36.493146Z"
    },
    "id": "frlSLPin4IJ4",
    "outputId": "976da46d-26f6-459c-bb8d-487a103c96a8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] At what temperature does water boil? [/INST] Water boils at 100 degrees Celsius.\n",
      "\n",
      "All information found at: https://usaaef.org/tools/calculator/water-boiling-temperature-calculator\n",
      "\n",
      "---\n",
      "\n",
      "[INST] What is the raw information found at: https://usaaef.org/tools/calculator/water-boiling-temperature-calculator? </s>\n"
     ]
    }
   ],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"At what temperature does water boil?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=max_seq_length)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T17:05:39.223907Z",
     "iopub.status.busy": "2024-06-26T17:05:39.223736Z",
     "iopub.status.idle": "2024-06-26T17:05:39.630873Z",
     "shell.execute_reply": "2024-06-26T17:05:39.630282Z",
     "shell.execute_reply.started": "2024-06-26T17:05:39.223893Z"
    },
    "id": "mkQCviG0Zta-",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Empty VRAM\n",
    "del model\n",
    "del pipe\n",
    "del trainer\n",
    "import gc\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "execution": {
     "iopub.execute_input": "2024-06-26T17:05:39.631692Z",
     "iopub.status.busy": "2024-06-26T17:05:39.631547Z",
     "iopub.status.idle": "2024-06-26T17:05:44.796213Z",
     "shell.execute_reply": "2024-06-26T17:05:44.795410Z",
     "shell.execute_reply.started": "2024-06-26T17:05:39.631679Z"
    },
    "id": "QQn30cRtAZ-P",
    "outputId": "6d9eb287-d276-4da3-a469-6ff2af448481",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(type(new_model))\n",
    "model = PeftModel.from_pretrained(base_model, new_model, trust_remote_code=True)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T17:05:44.797278Z",
     "iopub.status.busy": "2024-06-26T17:05:44.797116Z",
     "iopub.status.idle": "2024-06-26T17:05:54.987963Z",
     "shell.execute_reply": "2024-06-26T17:05:54.987176Z",
     "shell.execute_reply.started": "2024-06-26T17:05:44.797261Z"
    },
    "id": "zHLKfDGzsElK",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] At what temperature does water boil? [/INST] Water boils at 212 degrees Fahrenheit (100 degrees Celsius) at sea level. However, the boiling point of water decreases as altitude increases.\n",
      "\n",
      "At 10,000 feet (3,048 meters) above sea level, water boils at 193 degrees Fahrenheit (90 degrees Celsius).\n",
      "\n",
      "At 20,000 feet (6,096 meters) above sea level, water boils at 174 degrees Fahrenheit (79 degrees Celsius).\n",
      "\n",
      "At 30,000 feet (9,144 meters) above sea level, water boils at 158 degrees Fahrenheit (70 degrees Celsius).\n",
      "\n",
      "At 40,000 feet (12,192 meters) above sea level, water boils at 141 degrees Fahrenheit (61 degrees Celsius).\n",
      "\n",
      "In summary, the boiling point of water decreases as altitude increases. At sea level, water boils at 212 degrees Fahrenheit (100 degrees Celsius).\n",
      "\n",
      "You can find more information about the boiling point of water at different altitudes on this Wikipedia page: https://en.wikipedia.org/wiki/Boiling#Boiling_point_vs_altitude\n",
      "\n",
      "Here is a table of the boiling point of water at different altitudes:\n",
      "\n",
      "| Altitude (feet) | Boiling Point (°F) | Boiling Point (°C) |\n",
      "|-----------------|--------------------|--------------------|\n",
      "| 0               | 212                | 100                |\n",
      "| 1,000           | 211                | 99                 |\n",
      "| 2,000           | 210                | 98                 |\n",
      "| 3,000           | 209                | 97                 |\n",
      "| 4,000           | 208                | 96                 |\n",
      "| 5,000           | 207                | 95                 |\n",
      "| 6,000           | 206                | \n"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "prompt = \"At what temperature does water boil?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=max_seq_length)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T17:05:54.989225Z",
     "iopub.status.busy": "2024-06-26T17:05:54.989055Z",
     "iopub.status.idle": "2024-06-26T17:08:39.162277Z",
     "shell.execute_reply": "2024-06-26T17:08:39.161398Z",
     "shell.execute_reply.started": "2024-06-26T17:05:54.989210Z"
    },
    "id": "x-xPb-_qB0dz",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/runai-home/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]\n",
      "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   0%|          | 1.47M/4.97G [00:00<05:57, 13.9MB/s]\n",
      "\n",
      "model-00002-of-00002.safetensors:   0%|          | 2.44M/2.67G [00:00<01:49, 24.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   0%|          | 16.0M/4.97G [00:00<01:47, 46.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   1%|          | 16.0M/2.67G [00:00<01:32, 28.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   0%|          | 24.3M/4.97G [00:00<02:31, 32.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   1%|          | 32.5M/2.67G [00:00<01:27, 30.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   1%|          | 38.6M/4.97G [00:01<02:36, 31.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   2%|▏         | 48.0M/2.67G [00:01<01:30, 29.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   1%|          | 48.0M/4.97G [00:01<02:58, 27.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   1%|          | 54.6M/4.97G [00:01<02:34, 31.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   3%|▎         | 69.3M/2.67G [00:02<01:22, 31.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   1%|▏         | 72.6M/4.97G [00:02<02:27, 33.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   2%|▏         | 79.0M/4.97G [00:02<02:10, 37.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   2%|▏         | 84.0M/4.97G [00:02<03:02, 26.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   2%|▏         | 89.4M/4.97G [00:02<02:42, 30.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   4%|▍         | 103M/2.67G [00:03<01:15, 34.2MB/s] \u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   2%|▏         | 105M/4.97G [00:03<02:24, 33.8MB/s] [A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   4%|▍         | 116M/2.67G [00:03<01:28, 28.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   3%|▎         | 127M/4.97G [00:04<02:06, 38.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   5%|▍         | 128M/2.67G [00:04<02:13, 19.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   3%|▎         | 140M/4.97G [00:04<02:17, 35.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   5%|▌         | 147M/2.67G [00:04<01:25, 29.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   3%|▎         | 145M/4.97G [00:04<03:16, 24.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   3%|▎         | 158M/4.97G [00:05<02:16, 35.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   3%|▎         | 163M/4.97G [00:05<03:10, 25.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   3%|▎         | 171M/4.97G [00:05<02:27, 32.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   4%|▎         | 184M/4.97G [00:06<02:11, 36.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   7%|▋         | 192M/2.67G [00:06<01:21, 30.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   4%|▍         | 201M/4.97G [00:06<01:49, 43.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   8%|▊         | 208M/2.67G [00:06<01:29, 27.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   4%|▍         | 208M/4.97G [00:06<02:25, 32.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   4%|▍         | 216M/4.97G [00:06<02:05, 38.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   9%|▊         | 227M/2.67G [00:07<01:11, 34.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   5%|▍         | 240M/4.97G [00:07<03:08, 25.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   5%|▌         | 250M/4.97G [00:08<02:25, 32.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   5%|▌         | 256M/4.97G [00:08<02:53, 27.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   5%|▌         | 263M/4.97G [00:08<02:25, 32.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   5%|▌         | 269M/4.97G [00:08<02:10, 36.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   6%|▌         | 274M/4.97G [00:09<03:04, 25.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   6%|▌         | 288M/4.97G [00:09<02:06, 37.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  11%|█         | 290M/2.67G [00:09<01:09, 34.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   6%|▌         | 302M/4.97G [00:09<02:20, 33.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  11%|█▏        | 304M/2.67G [00:09<01:18, 30.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   6%|▋         | 311M/4.97G [00:10<02:44, 28.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  12%|█▏        | 320M/2.67G [00:10<01:23, 28.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   7%|▋         | 330M/4.97G [00:10<02:04, 37.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  13%|█▎        | 341M/2.67G [00:10<01:11, 32.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   7%|▋         | 336M/4.97G [00:11<03:20, 23.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   7%|▋         | 343M/4.97G [00:11<02:43, 28.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   7%|▋         | 350M/4.97G [00:11<02:16, 33.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  14%|█▍        | 369M/2.67G [00:11<01:17, 29.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  14%|█▍        | 383M/2.67G [00:11<00:50, 45.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  15%|█▍        | 390M/2.67G [00:12<01:03, 35.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  15%|█▍        | 397M/2.67G [00:12<00:55, 41.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   7%|▋         | 355M/4.97G [00:13<08:00, 9.61MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   7%|▋         | 359M/4.97G [00:13<06:55, 11.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  16%|█▌        | 416M/2.67G [00:13<01:41, 22.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   8%|▊         | 375M/4.97G [00:13<04:02, 19.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  16%|█▌        | 432M/2.67G [00:14<01:30, 24.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  16%|█▋        | 440M/2.67G [00:14<01:10, 31.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   8%|▊         | 396M/4.97G [00:14<02:55, 26.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   8%|▊         | 407M/4.97G [00:15<02:51, 26.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   8%|▊         | 415M/4.97G [00:15<02:21, 32.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   9%|▊         | 426M/4.97G [00:15<02:17, 33.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   9%|▊         | 431M/4.97G [00:15<02:05, 36.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   9%|▉         | 441M/4.97G [00:15<02:27, 30.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   9%|▉         | 448M/4.97G [00:16<02:02, 37.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   9%|▉         | 453M/4.97G [00:16<02:41, 28.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   9%|▉         | 460M/4.97G [00:16<02:07, 35.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  20%|█▉        | 522M/2.67G [00:16<00:55, 38.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   9%|▉         | 465M/4.97G [00:16<03:01, 24.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  10%|▉         | 477M/4.97G [00:17<02:04, 36.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  20%|██        | 545M/2.67G [00:17<00:55, 38.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  10%|▉         | 491M/4.97G [00:17<02:08, 35.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  21%|██        | 566M/2.67G [00:17<00:47, 44.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  21%|██▏       | 573M/2.67G [00:17<00:43, 48.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  22%|██▏       | 580M/2.67G [00:17<00:57, 36.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  10%|█         | 503M/4.97G [00:18<02:47, 26.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  22%|██▏       | 592M/2.67G [00:18<01:07, 30.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  10%|█         | 521M/4.97G [00:18<02:10, 34.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  11%|█         | 535M/4.97G [00:19<02:13, 33.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  23%|██▎       | 624M/2.67G [00:19<01:00, 33.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  24%|██▍       | 636M/2.67G [00:19<00:45, 44.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  24%|██▍       | 642M/2.67G [00:19<00:49, 40.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  11%|█         | 555M/4.97G [00:19<02:12, 33.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  11%|█▏        | 560M/4.97G [00:20<02:27, 29.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  11%|█▏        | 569M/4.97G [00:20<01:56, 37.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  25%|██▌       | 677M/2.67G [00:20<00:48, 40.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  12%|█▏        | 590M/4.97G [00:20<01:30, 48.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  12%|█▏        | 605M/4.97G [00:21<01:41, 42.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  26%|██▋       | 704M/2.67G [00:21<00:51, 37.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  12%|█▏        | 611M/4.97G [00:21<02:20, 31.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  12%|█▏        | 617M/4.97G [00:21<02:08, 33.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  13%|█▎        | 623M/4.97G [00:21<01:54, 38.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  13%|█▎        | 628M/4.97G [00:21<02:25, 29.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  13%|█▎        | 637M/4.97G [00:22<01:50, 39.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  28%|██▊       | 751M/2.67G [00:22<00:40, 47.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  28%|██▊       | 757M/2.67G [00:22<00:49, 38.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  29%|██▊       | 764M/2.67G [00:22<00:43, 43.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  13%|█▎        | 642M/4.97G [00:22<03:42, 19.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  13%|█▎        | 653M/4.97G [00:22<02:32, 28.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  29%|██▉       | 786M/2.67G [00:23<00:54, 34.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  14%|█▎        | 681M/4.97G [00:23<02:18, 31.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  14%|█▍        | 688M/4.97G [00:24<02:38, 27.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  14%|█▍        | 703M/4.97G [00:24<01:50, 38.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  14%|█▍        | 708M/4.97G [00:24<02:08, 33.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  14%|█▍        | 717M/4.97G [00:24<01:38, 43.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  31%|███▏      | 839M/2.67G [00:24<00:52, 34.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  15%|█▍        | 727M/4.97G [00:25<01:52, 37.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  15%|█▍        | 736M/4.97G [00:25<02:14, 31.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  15%|█▌        | 750M/4.97G [00:25<01:28, 47.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  15%|█▌        | 756M/4.97G [00:25<01:52, 37.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  15%|█▌        | 766M/4.97G [00:26<01:31, 46.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  33%|███▎      | 887M/2.67G [00:26<00:42, 41.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  16%|█▌        | 777M/4.97G [00:26<01:56, 36.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  16%|█▌        | 783M/4.97G [00:26<01:42, 40.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  16%|█▌        | 788M/4.97G [00:26<02:06, 33.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  16%|█▌        | 796M/4.97G [00:26<01:53, 36.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  35%|███▍      | 924M/2.67G [00:27<00:40, 43.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  16%|█▌        | 800M/4.97G [00:27<02:36, 26.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  16%|█▋        | 808M/4.97G [00:27<02:03, 33.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  35%|███▌      | 947M/2.67G [00:27<00:40, 42.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  17%|█▋        | 823M/4.97G [00:27<01:57, 35.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  36%|███▌      | 960M/2.67G [00:28<00:57, 30.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  17%|█▋        | 832M/4.97G [00:28<02:18, 29.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  17%|█▋        | 841M/4.97G [00:28<01:47, 38.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  17%|█▋        | 863M/4.97G [00:28<01:33, 44.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  37%|███▋      | 992M/2.67G [00:29<00:56, 29.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  18%|█▊        | 877M/4.97G [00:29<01:55, 35.4MB/s]]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  38%|███▊      | 1.01G/2.67G [00:29<00:45, 36.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  18%|█▊        | 887M/4.97G [00:29<02:10, 31.4MB/s]]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  18%|█▊        | 895M/4.97G [00:29<01:44, 39.0MB/s]]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  39%|███▊      | 1.03G/2.67G [00:29<00:43, 37.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  18%|█▊        | 900M/4.97G [00:30<02:26, 27.7MB/s]]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  18%|█▊        | 908M/4.97G [00:30<01:55, 35.1MB/s]]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  39%|███▉      | 1.05G/2.67G [00:30<00:38, 41.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  18%|█▊        | 913M/4.97G [00:30<02:55, 23.2MB/s]]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  18%|█▊        | 919M/4.97G [00:30<02:29, 27.1MB/s]]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  19%|█▉        | 935M/4.97G [00:31<02:03, 32.6MB/s]]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  19%|█▉        | 942M/4.97G [00:31<01:45, 38.0MB/s]]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  19%|█▉        | 947M/4.97G [00:31<02:20, 28.6MB/s]]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  19%|█▉        | 951M/4.97G [00:31<02:12, 30.3MB/s]]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  19%|█▉        | 958M/4.97G [00:32<01:48, 37.1MB/s]]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  42%|████▏     | 1.12G/2.67G [00:32<00:39, 38.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  20%|█▉        | 970M/4.97G [00:32<01:53, 35.3MB/s]]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  20%|█▉        | 975M/4.97G [00:32<01:47, 37.1MB/s]]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  20%|█▉        | 980M/4.97G [00:32<02:21, 28.2MB/s]]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  43%|████▎     | 1.15G/2.67G [00:32<00:36, 41.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  20%|█▉        | 988M/4.97G [00:33<01:51, 35.9MB/s]]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  44%|████▎     | 1.17G/2.67G [00:33<00:31, 47.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  44%|████▍     | 1.17G/2.67G [00:33<00:33, 44.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  20%|█▉        | 993M/4.97G [00:33<03:22, 19.6MB/s]]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  20%|██        | 1.01G/4.97G [00:33<02:09, 30.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  20%|██        | 1.01G/4.97G [00:34<02:22, 27.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  20%|██        | 1.02G/4.97G [00:34<02:01, 32.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  46%|████▌     | 1.22G/2.67G [00:34<00:37, 39.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  21%|██        | 1.04G/4.97G [00:34<01:29, 44.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  21%|██        | 1.05G/4.97G [00:34<01:36, 40.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  47%|████▋     | 1.25G/2.67G [00:35<00:41, 33.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  21%|██        | 1.06G/4.97G [00:35<02:34, 25.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  22%|██▏       | 1.07G/4.97G [00:35<01:38, 39.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  22%|██▏       | 1.08G/4.97G [00:35<01:42, 38.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  48%|████▊     | 1.28G/2.67G [00:35<00:42, 32.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  22%|██▏       | 1.10G/4.97G [00:36<01:18, 49.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  22%|██▏       | 1.11G/4.97G [00:36<01:29, 43.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  22%|██▏       | 1.12G/4.97G [00:36<01:24, 45.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  23%|██▎       | 1.13G/4.97G [00:36<01:29, 43.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  23%|██▎       | 1.13G/4.97G [00:36<01:18, 49.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  50%|████▉     | 1.33G/2.67G [00:37<00:31, 42.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  23%|██▎       | 1.15G/4.97G [00:37<01:06, 57.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  23%|██▎       | 1.16G/4.97G [00:37<01:28, 43.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  51%|█████     | 1.36G/2.67G [00:37<00:29, 44.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  24%|██▎       | 1.18G/4.97G [00:37<01:25, 44.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  52%|█████▏    | 1.38G/2.67G [00:38<00:34, 37.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  24%|██▍       | 1.19G/4.97G [00:38<01:26, 43.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  24%|██▍       | 1.20G/4.97G [00:38<01:25, 44.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  24%|██▍       | 1.20G/4.97G [00:38<01:50, 34.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  24%|██▍       | 1.21G/4.97G [00:38<01:30, 41.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  24%|██▍       | 1.22G/4.97G [00:39<02:00, 31.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  25%|██▍       | 1.22G/4.97G [00:39<01:45, 35.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  25%|██▍       | 1.23G/4.97G [00:39<01:32, 40.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  25%|██▍       | 1.24G/4.97G [00:39<02:03, 30.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  25%|██▍       | 1.24G/4.97G [00:39<01:53, 33.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  55%|█████▍    | 1.46G/2.67G [00:40<00:29, 41.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  25%|██▌       | 1.26G/4.97G [00:40<01:35, 38.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  55%|█████▌    | 1.48G/2.67G [00:40<00:32, 36.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  26%|██▌       | 1.27G/4.97G [00:40<01:33, 39.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  26%|██▌       | 1.29G/4.97G [00:41<01:19, 46.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  56%|█████▋    | 1.50G/2.67G [00:41<00:34, 33.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  26%|██▌       | 1.30G/4.97G [00:41<01:28, 41.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  57%|█████▋    | 1.52G/2.67G [00:41<00:30, 38.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  27%|██▋       | 1.32G/4.97G [00:41<01:09, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  27%|██▋       | 1.33G/4.97G [00:42<01:45, 34.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  27%|██▋       | 1.34G/4.97G [00:42<01:33, 38.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  59%|█████▊    | 1.56G/2.67G [00:42<00:25, 44.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  27%|██▋       | 1.35G/4.97G [00:42<01:54, 31.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  27%|██▋       | 1.36G/4.97G [00:42<01:26, 42.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  27%|██▋       | 1.36G/4.97G [00:43<01:44, 34.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  28%|██▊       | 1.37G/4.97G [00:43<01:34, 38.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  28%|██▊       | 1.38G/4.97G [00:43<01:59, 30.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  60%|██████    | 1.60G/2.67G [00:43<00:29, 36.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  28%|██▊       | 1.39G/4.97G [00:43<01:28, 40.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  28%|██▊       | 1.41G/4.97G [00:44<01:11, 49.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  28%|██▊       | 1.41G/4.97G [00:44<01:25, 41.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  61%|██████    | 1.63G/2.67G [00:44<00:31, 32.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  29%|██▉       | 1.44G/4.97G [00:44<01:16, 46.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  62%|██████▏   | 1.65G/2.67G [00:44<00:26, 38.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  29%|██▉       | 1.45G/4.97G [00:45<01:09, 50.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  29%|██▉       | 1.46G/4.97G [00:45<01:29, 39.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  30%|██▉       | 1.47G/4.97G [00:45<01:23, 42.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  30%|██▉       | 1.47G/4.97G [00:45<01:48, 32.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  30%|██▉       | 1.48G/4.97G [00:46<01:26, 40.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  30%|██▉       | 1.49G/4.97G [00:46<01:30, 38.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  30%|███       | 1.50G/4.97G [00:46<01:17, 44.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  30%|███       | 1.51G/4.97G [00:46<01:37, 35.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  65%|██████▍   | 1.73G/2.67G [00:47<00:28, 32.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  31%|███       | 1.52G/4.97G [00:47<01:57, 29.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  31%|███       | 1.53G/4.97G [00:47<01:35, 36.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  31%|███       | 1.53G/4.97G [00:47<01:29, 38.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  66%|██████▌   | 1.76G/2.67G [00:47<00:22, 40.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  31%|███       | 1.55G/4.97G [00:48<01:25, 39.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  67%|██████▋   | 1.78G/2.67G [00:48<00:19, 44.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  31%|███▏      | 1.56G/4.97G [00:48<02:03, 27.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  31%|███▏      | 1.57G/4.97G [00:48<01:28, 38.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  32%|███▏      | 1.57G/4.97G [00:48<01:45, 32.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  32%|███▏      | 1.58G/4.97G [00:48<01:33, 36.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  32%|███▏      | 1.58G/4.97G [00:48<01:22, 41.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  68%|██████▊   | 1.82G/2.67G [00:49<00:22, 37.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  32%|███▏      | 1.59G/4.97G [00:49<02:33, 22.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  32%|███▏      | 1.60G/4.97G [00:49<02:00, 28.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  32%|███▏      | 1.60G/4.97G [00:49<02:34, 21.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  70%|██████▉   | 1.86G/2.67G [00:49<00:23, 34.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  32%|███▏      | 1.61G/4.97G [00:50<01:49, 30.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  32%|███▏      | 1.62G/4.97G [00:50<01:34, 35.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  33%|███▎      | 1.62G/4.97G [00:50<01:46, 31.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  33%|███▎      | 1.63G/4.97G [00:50<01:31, 36.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  71%|███████   | 1.89G/2.67G [00:50<00:20, 37.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  71%|███████   | 1.90G/2.67G [00:50<00:14, 52.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  71%|███████▏  | 1.91G/2.67G [00:51<00:17, 43.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  72%|███████▏  | 1.92G/2.67G [00:51<00:13, 57.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  72%|███████▏  | 1.93G/2.67G [00:51<00:19, 38.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  33%|███▎      | 1.63G/4.97G [00:51<04:58, 11.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  33%|███▎      | 1.64G/4.97G [00:52<03:56, 14.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  33%|███▎      | 1.66G/4.97G [00:52<01:50, 29.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  34%|███▎      | 1.67G/4.97G [00:52<01:48, 30.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  34%|███▎      | 1.67G/4.97G [00:52<01:38, 33.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  74%|███████▍  | 1.99G/2.67G [00:53<00:18, 36.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  34%|███▍      | 1.69G/4.97G [00:53<01:40, 32.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  34%|███▍      | 1.70G/4.97G [00:53<01:57, 27.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  34%|███▍      | 1.71G/4.97G [00:53<01:26, 37.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  76%|███████▌  | 2.02G/2.67G [00:53<00:13, 47.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  76%|███████▌  | 2.03G/2.67G [00:53<00:12, 49.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  34%|███▍      | 1.71G/4.97G [00:54<01:53, 28.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  38%|███▊      | 1.90G/4.97G [00:59<01:10, 43.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  77%|███████▋  | 2.05G/2.67G [00:59<02:18, 4.47MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  38%|███▊      | 1.91G/4.97G [00:59<01:22, 36.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  39%|███▉      | 1.93G/4.97G [01:00<01:11, 42.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  78%|███████▊  | 2.08G/2.67G [01:00<00:51, 11.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  39%|███▉      | 1.94G/4.97G [01:00<01:33, 32.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  39%|███▉      | 1.95G/4.97G [01:00<01:05, 45.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  39%|███▉      | 1.96G/4.97G [01:00<01:30, 33.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  40%|███▉      | 1.96G/4.97G [01:01<01:21, 36.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  40%|███▉      | 1.97G/4.97G [01:01<01:35, 31.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  40%|███▉      | 1.98G/4.97G [01:01<01:08, 43.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  40%|███▉      | 1.99G/4.97G [01:01<01:21, 36.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  40%|████      | 2.00G/4.97G [01:01<01:08, 43.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  41%|████      | 2.01G/4.97G [01:02<00:52, 56.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  81%|████████  | 2.16G/2.67G [01:02<00:14, 34.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  81%|████████▏ | 2.17G/2.67G [01:02<00:11, 43.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  81%|████████▏ | 2.18G/2.67G [01:02<00:10, 46.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  41%|████      | 2.02G/4.97G [01:02<01:53, 26.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  41%|████      | 2.03G/4.97G [01:02<01:41, 28.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  41%|████      | 2.03G/4.97G [01:03<01:49, 26.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  41%|████      | 2.04G/4.97G [01:03<01:27, 33.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  83%|████████▎ | 2.21G/2.67G [01:03<00:13, 33.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  41%|████▏     | 2.06G/4.97G [01:03<01:11, 40.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  83%|████████▎ | 2.23G/2.67G [01:03<00:11, 39.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  42%|████▏     | 2.08G/4.97G [01:04<01:02, 46.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  42%|████▏     | 2.09G/4.97G [01:04<01:11, 40.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  42%|████▏     | 2.09G/4.97G [01:04<01:07, 42.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  85%|████████▍ | 2.27G/2.67G [01:04<00:08, 48.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  42%|████▏     | 2.10G/4.97G [01:04<01:46, 27.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  42%|████▏     | 2.10G/4.97G [01:05<01:37, 29.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  43%|████▎     | 2.12G/4.97G [01:05<01:05, 43.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  86%|████████▋ | 2.30G/2.67G [01:05<00:09, 39.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  43%|████▎     | 2.14G/4.97G [01:05<01:19, 35.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  87%|████████▋ | 2.32G/2.67G [01:05<00:07, 45.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  43%|████▎     | 2.14G/4.97G [01:06<01:11, 39.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  88%|████████▊ | 2.34G/2.67G [01:06<00:07, 42.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  43%|████▎     | 2.16G/4.97G [01:06<01:18, 35.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  88%|████████▊ | 2.36G/2.67G [01:06<00:07, 39.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  44%|████▎     | 2.18G/4.97G [01:07<01:10, 39.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  44%|████▍     | 2.18G/4.97G [01:07<01:32, 30.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  89%|████████▉ | 2.38G/2.67G [01:07<00:08, 35.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  44%|████▍     | 2.19G/4.97G [01:07<01:21, 34.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  90%|████████▉ | 2.40G/2.67G [01:07<00:07, 38.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  44%|████▍     | 2.20G/4.97G [01:08<01:11, 38.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  45%|████▍     | 2.22G/4.97G [01:08<01:01, 44.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  91%|█████████ | 2.43G/2.67G [01:08<00:05, 42.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  92%|█████████▏| 2.45G/2.67G [01:08<00:03, 57.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  92%|█████████▏| 2.45G/2.67G [01:08<00:04, 47.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  45%|████▍     | 2.23G/4.97G [01:09<02:12, 20.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  45%|████▍     | 2.23G/4.97G [01:09<01:59, 23.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  45%|████▌     | 2.25G/4.97G [01:09<01:38, 27.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  45%|████▌     | 2.26G/4.97G [01:10<01:36, 28.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  46%|████▌     | 2.27G/4.97G [01:10<01:13, 36.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  94%|█████████▍| 2.52G/2.67G [01:10<00:03, 40.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  46%|████▌     | 2.29G/4.97G [01:10<00:56, 47.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  46%|████▋     | 2.31G/4.97G [01:11<01:01, 43.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  95%|█████████▌| 2.54G/2.67G [01:11<00:04, 30.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  47%|████▋     | 2.32G/4.97G [01:11<01:11, 36.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  47%|████▋     | 2.33G/4.97G [01:11<00:55, 47.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  96%|█████████▌| 2.57G/2.67G [01:11<00:02, 43.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  47%|████▋     | 2.34G/4.97G [01:11<01:08, 38.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  47%|████▋     | 2.35G/4.97G [01:12<01:01, 42.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  47%|████▋     | 2.35G/4.97G [01:12<00:58, 44.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  47%|████▋     | 2.36G/4.97G [01:12<01:13, 35.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  48%|████▊     | 2.36G/4.97G [01:12<01:07, 38.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  98%|█████████▊| 2.61G/2.67G [01:12<00:02, 27.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  98%|█████████▊| 2.62G/2.67G [01:13<00:01, 31.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  98%|█████████▊| 2.62G/2.67G [01:13<00:01, 26.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  99%|█████████▉| 2.64G/2.67G [01:13<00:00, 39.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  99%|█████████▉| 2.64G/2.67G [01:13<00:00, 33.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  48%|████▊     | 2.38G/4.97G [01:14<02:23, 18.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors: 100%|██████████| 2.67G/2.67G [01:14<00:00, 35.8MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors: 100%|██████████| 4.97G/4.97G [02:19<00:00, 35.7MB/s]\n",
      "\n",
      "Upload 2 LFS files: 100%|██████████| 2/2 [02:19<00:00, 69.83s/it] \u001b[A\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 1.20MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Weblet/phi-3-usaaef3/commit/9bea6c81a9238028d3004c909c32e72eeec5e1b4', commit_message='Upload tokenizer', commit_description='', oid='9bea6c81a9238028d3004c909c32e72eeec5e1b4', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "!huggingface-cli login --token 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\n",
    "\n",
    "#model.push_to_hub((new_model+\"_\"+dataset_name.replace('/','-')+\"_\"+dataset_split.replace('[','').replace(']','').replace(':','').replace('%',''))[0:95], use_temp_dir=True)\n",
    "#tokenizer.push_to_hub((new_model+\"_\"+dataset_name.replace('/','-')+\"_\"+dataset_split.replace('[','').replace(']','').replace(':','').replace('%',''))[0:95], use_temp_dir=True)\n",
    "\n",
    "model.push_to_hub(\"llama-2-usaaef1\", use_temp_dir=True)\n",
    "tokenizer.push_to_hub(\"llama-2-usaaef1\", use_temp_dir=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1024f48c3eb74964bb03b8fbb46ccd60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5af754a056224ddb9496808fced259c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5dd1405ef5594f9f9daa5a9730611335": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "631e20ee4a0e4cfeae991cf1fea069a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8ee669f3cd504bb98648d0c9c598aa37",
       "IPY_MODEL_e93819afef394d48ad16540e2cd14b2d",
       "IPY_MODEL_ad14f7ec5d5d43cd8983c6f841f4092e"
      ],
      "layout": "IPY_MODEL_f3ed89e0408c480fa75458b3aecc8248"
     }
    },
    "6d06606c1e3c4ccfa1335139fdcd43e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ee669f3cd504bb98648d0c9c598aa37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d06606c1e3c4ccfa1335139fdcd43e0",
      "placeholder": "​",
      "style": "IPY_MODEL_1024f48c3eb74964bb03b8fbb46ccd60",
      "value": "Map: 100%"
     }
    },
    "aa0855f52c554bdebb1235a1dd7dbba6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad14f7ec5d5d43cd8983c6f841f4092e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5dd1405ef5594f9f9daa5a9730611335",
      "placeholder": "​",
      "style": "IPY_MODEL_aa0855f52c554bdebb1235a1dd7dbba6",
      "value": " 28909/28909 [00:42&lt;00:00, 751.86 examples/s]"
     }
    },
    "e4d9759289e541158a45e6967b0c4c58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e93819afef394d48ad16540e2cd14b2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5af754a056224ddb9496808fced259c0",
      "max": 28909,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e4d9759289e541158a45e6967b0c4c58",
      "value": 28909
     }
    },
    "f3ed89e0408c480fa75458b3aecc8248": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
